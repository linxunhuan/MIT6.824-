# Abstract
+ MapReduce 是一种编程模型和相关实现，用于处理和生成大型数据集
  + 用户指定一个 map 函数来处理键/值对以生成一组中间键/值对
  + 以及一个 Reduce 函数来合并与同一中间键相关联的所有中间值
  + 许现实世界的任务都可以用这个模型来表达
+ 以这种函数式风格编写的程序会自动并行化并在大型商用机器集群上执行
  + 运行时系统负责处理输入数据分区、在一组机器上安排程序执行、处理机器故障以及管理所需的机器间通信等细节
  + 这使得,没有任何并行和分布式系统经验的程序员,可以轻松利用大型分布式系统的资源
+ MapReduce 实现运行在大型商用机器集群上，并且具有高度可扩展性：
  + 典型的 MapReduce 计算在数千台机器上处理数 TB 的数据
  + 程序员发现该系统易于使用：
    + 数百个 MapReduce 程序已经实现
    + 每天在 Google 集群上执行的 MapReduce 作业多达一千个
#   Programming Model
+ 计算需要一组输入键/值对，并且生成一组输出键/值对
+ MapReduce 库的用户将计算表示为两个函数：Map 和 Reduce
  + Map 由用户编写，接受一个输入对并生成一组中间键/值对
    + MapReduce 库将与同一中间键 I 相关联的所有中间值组合在一起，并将它们传递给 Reduce 函数
  + Reduce 函数也是由用户编写的，接受一个中间键 I 和该键的一组值
    + 它将这些值合并在一起以形成一个可能更小的值集
    + 通常，每次 Reduce 调用只生成零个或一个输出值
    + 中间值通过迭代器提供给用户的 Reduce 函数
    + 这使我们能够处理太大而无法放入内存的值列表
## more examples
几个有趣的程序的简单示例，可以很容易地表示为 MapReduce 计算 
+ 分布式 Grep：
  + 如果 map 函数与提供的模式匹配，则它会发出一行
  + reduce 函数是一个身份函数，它只是将提供的中间数据复制到输出
+ URL 访问频率计数：
  + map 函数处理网页请求日志并输出(URL,1)
  + reduce 函数将同一 URL 的所有值相加，并发出(URL,total count)
...

# Implementation
+ MapReduce 接口有许多不同的实现
  + 例如，一种实现可能适用于小型共享内存机器，另一种实现适用于大型 NUMA 多处理器，还有一种实现适用于更大的联网机器集合
+ 本节描述了一种针对 Google 广泛使用的计算环境的实现：
  + 通过交换以太网连接在一起的大型商用 PC 集群
  + 在我们的环境中：
        (1) 机器通常是双处理器 x86 处理器，运行 Linux，每台机器有 2-4 GB 内存
        (2) 使用商用网络硬件
            通常机器级别为 100 兆比特/秒或 1 千兆比特/秒，但总体二分带宽平均值要低得多
        (3) 集群由数百或数千台机器组成，因此机器故障很常见
        (4) 存储由直接连接到各个机器的廉价 IDE 磁盘提供
            内部开发的分布式文件系统用于管理存储在这些磁盘上的数据文件系统使用复制来在不可靠的
            硬件上提供可用性和可靠性
        (5) 用户将作业提交给调度系统
            每个作业包含一组任务，并由调度程序映射到集群内的一组可用机器
## Execution Overview
+ 通过自动对输入数据进行分区，Map 调用分布在多台机器上分成一组 M 个分割
+ 输入分割可由不同的机器并行处理
+ Reduce 调用通过使用分区函数（例如hash(key) mod R）将中间键空间划分为 R 个部分来分布
+ 分区数 (R) 和分区函数由用户指定
  + 图 显示了我们实现中 MapReduce 操作的总体流程
  + 当用户程序调用 MapReduce 函数时，将发生以下操作序列
<img src=".\picture\image.png">

（图中的编号标签对应于下面列表中的数字）：
1. 用户程序中的 MapReduce 库首先将输入文件分割成 M 个部分，每个部分通常为 16 兆字节到 64 兆字节 (MB)（用户可通过可选参数控制）
    然后，它在一组机器上启动该程序的许多副本 
2. 程序的一个副本很特殊，即主程序
    主程序负责管理整个过程，包括其余的都是由主程序分配工作的工作者
    有 M 个 map 任务和 R 个 reduce
    任务需要分配。主程序挑选空闲的工作者，并为每个工作者分配一个 map 任务或一个 reduce 任务 
3. 被分配 map 任务的工作者读取相应输入分割的内容
    它从输入数据中解析出键/值对，并将每个对传递给用户定义的 Map 函数
    Map 函数生成的中间键/值对缓冲在内存中 
4. 定期将缓冲对写入本地磁盘，并由分区函数将其分区为 R 个区域
    这些缓冲对在本地磁盘上的位置被传回主服务器，主服务器负责将这些位置转发给缩减工作进程 
5. 当缩减工作进程收到主服务器关于这些位置的通知时，它会使用远程过程调用从映射工作进程的本地磁盘读取缓冲数据
    当缩减工作进程读取所有中间数据后，它会按中间键对数据进行排序，以便将相同键的所有出现组合在一起
    排序是必需的，因为通常许多不同的键映射到同一个缩减任务
    如果中间数据量太大而无法放入内存中，则使用外部排序 
6. 缩减工作进程对已排序的中间数据进行迭代
    对于遇到的每个唯一中间键，它会将键和相应的中间值集传递给用户的 Reduce 函数
    Reduce 函数的输出被附加到此缩减分区的最终输出文件中
7. 当所有map任务和reduce任务都完成后，master唤醒用户程序
    此时，用户程序中的MapReduce调用返回到用户代码
+ 成功完成后，mapreduce 执行的输出可在 R 输出文件中获得（每个 reduce 任务一个，文件名由用户指定）
  + 通常，用户不需要将这些 R 输出文件合并为一个文件
  + 他们经常将这些文件作为输入传递给另一个 MapReduce 调用
  + 或者从另一个能够处理被分割成多个文件的输入的分布式应用程序中使用它们
## 主数据结构
+ 主服务器保留多个数据结构
+ 对于每个映射任务和化简任务，它存储状态（空闲、进行中、或已完成）和工作机器的标识（对于非空闲任务）
+ 主服务器是将中间文件区域的位置从映射任务传播到化简任务的管道
  + 因此，对于每个已完成的映射任务
  + 主服务器存储由映射任务生成的 R 个中间文件区域的位置和大小
  + 在完成映射任务时，将收到对此位置和大小信息的更新
  + 该信息将逐步推送到具有进行中的化简任务的工作服务器
## Fault Tolerance
由于 MapReduce 库旨在使用数百或数千台机器处理大量数据，因此该库必须能够优雅地容忍机器故障
## Worker Failure
+ 主节点定期 ping 每个工作节点
  + 如果在一定时间内未收到工作节点的响应，主节点会将该工作节点标记为失败
  + 工作节点完成的所有映射任务都会重置为初始空闲状态，因此可以调度到其他工作节点
  + 同样，失败工作节点上正在进行的任何映射任务或减少任务也会重置为空闲状态，并且可以重新调度
+ 已完成的 map 任务在发生故障时会重新执行
  + 因为其输出存储在发生故障的机器的本地磁盘上，因此无法访问
  + 已完成的reduce 任务不需要重新执行，因为其输出存储在全局文件系统中
+ 当一个 map 任务首先由 worker A 执行，然后由 worker B 执行（因为 A 失败）
  + 所有执行 Reduce 任务的 worker 会收到重新执行的通知
  + 任何尚未从 worker A 读取数据的 Reduce 任务都将从 worker B 读取数据
+ MapReduce 能够应对大规模工作程序故障
  + 例如，在一次 MapReduce 操作期间，正在运行的集群上的网络维护导致 80 台机器同时无法访问几分钟
  + MapReduce 主服务器只需重新​​执行无法访问的工作程序机器完成的工作
  + 然后继续前进，最终完成 MapReduce 操作
## Master Failure
+ 很容易让主服务器定期写入上述主数据结构的检查点
  + 如果主任务死亡，可以从最后一个检查点状态开始新的副本
    + 但是，鉴于只有一个主服务器，它发生故障的可能性不大
    + 因此，如果主服务器发生故障，我们当前的实现将中止 MapReduce 计算 
+ 客户端可以检查此情况,并根据需要重试 MapReduce 操作

# Conclusions
+ MapReduce 编程模型已在 Google 中成功用于多种用途
+ 我们认为这一成功有以下几个原因
  + 首先，该模型易于使用，即使对于没有并行和分布式系统经验的程序员也是如此
    + 因为它隐藏了并行化、容错、局部性优化和负载平衡的细节
  + 其次，各种各样的问题都可以轻松地用 MapReduce 计算来表达
    + 例如，MapReduce 用于为 Google 的生产网络搜索服务生成数据、进行排序、进行数据挖掘、进行机器学习和许多其他系统
  + 第三，我们开发了一种 MapReduce 实现
    + 该实现可扩展到由数千台机器组成的大型机器集群
    + 该实现高效利用了这些机器资源，因此适用于解决 Google 遇到的许多大型计算问题
+ 我们从这项工作中学到了很多东西
  + 首先，限制编程模型可以轻松实现并行化和分布式计算，并使此类计算具有容错能力
  + 其次，网络带宽是一种稀缺资源
    + 因此，我们的系统中的许多优化都旨在减少通过网络发送的数据量
      + 局部性优化允许我们从本地磁盘读取数据，而将中间数据的单个副本写入本地磁盘可节省网络带宽
  + 第三，可以使用冗余执行来减少速度慢的机器的影响，并处理机器故障和数据丢失
