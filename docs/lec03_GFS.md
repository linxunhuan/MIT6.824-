# 分布式存储系统的难点
为什么分布式存储系统会如此之难，以至于你需要做大量的工作才能让它正确工作？
+ 人们设计大型分布式系统或大型存储系统出发点通常是，他们想获取巨大的性能加成，进而利用数百台计算机的资源来同时完成大量工作
+ 因此，性能问题就成为了最初的诉求
  + 之后，很自然的想法就是将数据分割放到大量的服务器上
  + 这样就可以并行的从多台服务器读取数据
  + **我们将这种方式称之为分片（Sharding）**
+ 如果在成百上千台服务器进行分片，将会看见常态的故障
  + 如果有数千台服务器，那么总是会有一台服务器宕机，每天甚至每个小时都可能会发生错误
  + 所以，我们需要自动化的方法而不是人工介入来修复错误
  + **我们需要一个自动的容错系统，这就引出了容错这个话题（fault tolerance）**
+ 实现容错最有用的一种方法是使用复制
  + 只需要维护2-3个数据的副本
  + 当其中一个故障了，你就可以使用另一个
  + **所以，如果想要容错能力，就得有复制（replication）**
+ 如果有复制，那就有了两份数据的副本
  + 但是如果你不够小心，两个数据的副本就不是完全一致，严格来说，它们就不再互为副本了
  + 而你获取到的数据内容也将取决于你向哪个副本请求数据
  + **所以，如果我们有了复制，我们就有不一致的问题（inconsistency）**
+ 通过聪明的设计，你可以避免不一致的问题，并且让数据看起来也表现的符合预期
  + 但是为了达到这样的效果，你总是需要额外的工作
  + 需要不同服务器之间通过网络额外的交互，而这样的交互会降低性能
  + 所以如果你想要一致性，你的代价就是低性能。但这明显不是我们最开始所希望的
# 错误的设计
+ 对于具备强一致或者好的一致性的系统，从应用程序或者客户端看起来就像是和一台服务器在通信
  + 你可以认为只有一台服务器，甚至这个服务器只运行单线程，它同一时间只处理来自客户端的一个请求
  + 可能会有大量的客户端并发的发送请求到服务器上
    + 这里要求服务器从请求中挑选一个出来先执行，执行完成之后再执行下一个
+ 对于存储服务器来说，它上面会有一块磁盘
  + 执行一个写请求或许意味着向磁盘写入一个数据或者对数据做一次自增
  + 如果这是一次修改操作，并且我们有一个以key-value为索引的数据表单，那么我们会修改这个表单
  + 如果是一次读取操作，我们只需要将之前写入的数据，从表单中取出即可
<img src=".\picture\image1.png">

+ 为了让这里的简单服务有可预期的行为，需要定义一条规则：
  + 一个时间只执行一条请求
  + 这样每个请求都可以看到之前所有请求按照顺序执行生成的数据
  + 所以，如果有一些写请求，并且服务器以某种顺序一次一个的处理了它们，当你从服务器读数据时，你可以看到期望的数据
    + 例如，我们有一些客户端，客户端C1发起写请求将X设置成1
    + 在同一时刻，客户端C2发起写请求将X设置成2
    + 过了一会，在C1和C2的写请求都执行完毕之后，客户端C3会发送读取X的请求，并得到了一个结果
    + 客户端C4也会发送读取X的请求，也得到了一个结果。现在的问题是，这两个客户端看到的结果是什么？
+ 有两台服务器，每个服务器都有数据的一份完整拷贝
  + 它们在磁盘上都存储了一个key-value表单
  + 我们希望这两个表单是完全一致的，这样，一台服务器故障了，我们可以切换到另一台服务器去做读写
  + 两个表单完全一致意味着，每一个写请求都必须在两台服务器上执行
    + 而读请求只需要在一台服务器上执行，否则就没有容错性了
      + 因为如果读请求也需要从两台服务器读数据，那么一台服务器故障我们就没法提供服务了
  + 假设客户端C1和C2都想执行写请求，其中一个要写X为1，另一个写X为2
    + C1会将写X为1的请求发送个两个服务器，因为我们想要更新两台服务器上的数据
    + C2也会将写X为2的请求发送给两个服务器
+ 我们没有做任何事情来保障两台服务器以相同的顺序处理这2个请求
  + 如果服务器1（S1）先处理C1的请求，那么在它的表单里面，X先是1，之后S1看到了来自C2的请求，会将自己表单中的X覆盖成2
  + 如果S2恰好以不同的顺序收到客户端请求，那么S2会先执行C2的请求，将X设置为2，之后收到C1的请求，将X设置为1
<img src=".\picture\image2.png">

+ 假设C3从S1读数据，C4从S2读数据，我们就会面临一个可怕的场景：
  + 这两个客户端读取的数据不一样
  + 但是从前一个例子中的简单模型可以看出，相连的读请求应该读出相同的数据
+ 假设我们尝试修复上面的问题
  + 我们让客户端在S1还在线的时候，只从S1读取数据，S1不在线了再从S2读取数据
  + 这样最开始所有的客户端读X都能得到2
  + 但是突然，如果S1故障了，尽管没有写请求将X改成1，客户端读X得到的数据将会从2变成1
    + 因为S1故障之后，所有的客户端都会切换到S2去读数据
    + 这种数据的神奇变化与任何写操作都没有关联，并且也不可能在前一个例子的简单模型中发生
+ 这里的问题是可以修复的，修复需要服务器之间更多的通信，并且复杂度也会提升
  + 由于获取强一致会带来不可避免的复杂性的提升，有大量的方法可以在好的一致性和一些小瑕疵行为之间追求一个平衡
# GFS的设计目标
+ Google的目标是构建一个大型的，快速的文件系统
  + 并且这个文件系统是全局有效的，这样各种不同的应用程序都可以从中读取数据
+ 一种构建大型存储系统的方法是针对某个特定的应用程序构建特定的裁剪的存储系统
  + 但是如果另一个应用程序也想要一个大型存储系统，那么又需要重新构建一个存储系统
+ 如果有一个全局通用的存储系统
  + 那就意味着如果存储了大量从互联网抓取的数据，你也可以通过申请权限来查看这些数据
  + 这样，任何在Google内部的人员都可以根据名字读取这个文件系统（GFS）中可被共享的内容
+ 为了获得大容量和高速的特性，每个包含了数据的文件会被GFS自动的分割并存放在多个服务器之上
  + 这样读写操作自然就会变得很快
  + 因为可以从多个服务器上同时读取同一个文件，进而获得更高的聚合吞吐量
  + 将文件分割存储还可以在存储系统中保存比单个磁盘还要大的文件
+ 还有一些特征并非是设计目标
  + 比如GFS被设计成只在一个数据中心运行，所以这里并没有将副本保存在世界各地
    + 单个GFS只存在于单个数据中心的单个机房里
  + 其次，GFS并不面向普通的用户，这是一个Google内部使用的系统，供Google的工程师写应用程序使用
  + 第三，GFS在各个方面对大型的顺序文件读写做了定制
    + 在存储系统中有一个完全不同的领域，这个领域只对小份数据进行优化
      + 例如一个银行账户系统就需要一个能够读写100字节的数据库，因为100字节就可以表示人们的银行账户
    + 但是GFS不是这样的系统，GFS是为TB级别的文件而生
      + 并且GFS只会顺序处理，不支持随机访问
      + 某种程度上来说，它有点像批处理的风格
    + GFS并没有花费过多的精力来降低延迟，它的关注点在于巨大的吞吐量上，所以单次操作都涉及到MB级别的数据 
# GFS Master 节点
+ 假设我们有上百个客户端和一个Master节点
  + 尽管实际中可以拿多台机器作为Master节点
  + 但是GFS中Master是Active-Standby模式，所以只有一个Master节点在工作
+ Master节点保存了文件名和存储位置的对应关系
  + 除此之外，还有大量的Chunk服务器，可能会有数百个，每一个Chunk服务器上都有1-2块磁盘
<img src=".\picture\image3.png">

+ 在这里，**Master节点用来管理文件和Chunk的信息，而Chunk服务器用来存储实际的数据**
  + 这是GFS设计中比较好的一面
  + 它将这两类数据的管理问题几乎完全隔离开了，这样这两个问题可以使用独立设计来解决
+ Master节点知道每一个文件对应的所有的Chunk的ID
  + 这些Chunk每个是64MB大小
  + 它们共同构成了一个文件
+ 如果我有一个1GB的文件，当我想读取这个文件中的任意一个部分时
  + 我需要向Master节点查询对应的Chunk在哪个服务器上
  + 之后我可以直接从Chunk服务器读取对应的Chunk数据
+ 更进一步，我们看一下GFS的一致性以及GFS是如何处理故障
  + 为了了解这些，我们需要知道Master节点内保存的数据内容
  + 这里我们关心的主要是两个表单：
    + 第一个是文件名到Chunk ID或者Chunk Handle数组的对应
      + 这个表单告诉你，文件对应了哪些Chunk
      + 但是只有Chunk ID是做不了太多事情的，所以有了第二个表单
    + 第二个表单记录了Chunk ID到Chunk数据的对应关系
      + 这里的数据又包括了：
        + 每个Chunk存储在哪些服务器上，所以这部分是Chunk服务器的列表
        + 每个Chunk当前的版本号，所以Master节点必须记住每个Chunk对应的版本号
  + 所有对于Chunk的写操作都必须在主Chunk（Primary Chunk）上顺序处理，主Chunk是Chunk的多个副本之一
    + 所以，Master节点必须记住哪个Chunk服务器持有主Chunk
    + 并且，主Chunk只能在特定的租约时间内担任主Chunk
      + 所以，Master节点要记住主Chunk的租约过期时间
+ 以上数据都存储在内存中，如果Master故障了，这些数据就都丢失了
  + 为了能让Master重启而不丢失数据，Master节点会同时将数据存储在磁盘上
  + 所以Master节点读数据只会从内存读，但是写数据的时候，至少有一部分数据会接入到磁盘中
  + 更具体来说，Master会在磁盘上存储log
    + 每次有数据变更时，Master会在磁盘的log中追加一条记录
    + 并生成CheckPoint（类似于备份点）
+ 有些数据需要存在磁盘上，而有些不用。它们分别是：
  + Chunk Handle的数组（第一个表单）要保存在磁盘上
    + 给它标记成NV（non-volatile, 非易失），这个标记表示对应的数据会写入到磁盘上
  + Chunk服务器列表不用保存到磁盘上
    + 因为Master节点重启之后可以与所有的Chunk服务器通信，并查询每个Chunk服务器存储了哪些Chunk
    + 所以这里标记成V（volatile）
  + 版本号要不要写入磁盘取决于GFS是如何工作的，这里先标记成NV
  + 主Chunk的ID，几乎可以确定不用写入磁盘
    + 因为Master节点重启之后会忘记谁是主Chunk，它只需要等待60秒租约到期，那么它知道对于这个Chunk来说没有主Chunk
    + 这个时候，Master节点可以安全指定一个新的主Chunk
    + 所以这里标记成V
  + 类似的，租约过期时间也不用写入磁盘，所以这里标记成V
+ 任何时候，如果文件扩展到达了一个新的64MB，需要新增一个Chunk或者由于指定了新的主Chunk而导致版本号更新了
  + Master节点需要向磁盘中的Log追加一条记录说
    + 我刚刚向这个文件添加了一个新的Chunk或者我刚刚修改了Chunk的版本号
  + 所以每次有这样的更新，都需要写磁盘
  + GFS论文并没有讨论这么多细节，但是因为写磁盘的速度是有限的
    + 写磁盘会导致Master节点的更新速度也是有限的，所以要尽可能少的写入数据到磁盘
+ 这里在磁盘中维护log而不是数据库的原因是
  + 数据库本质上来说是某种B树（b-tree）或者hash table
  + 相比之下，追加log会非常的高效
    + 因为你可以将最近的多个log记录一次性的写入磁盘
    + 因为这些数据都是向同一个地址追加，这样只需要等待磁盘的磁碟旋转一次
  + 而对于B树来说，每一份数据都需要在磁盘中随机找个位置写入
    + 所以使用Log可以使得磁盘写入更快一些
+ 当Master节点故障重启，并重建它的状态，你不会想要从log的最开始重建状态
  + 因为log的最开始可能是几年之前，所以Master节点会在磁盘中创建一些checkpoint点
  + 这样Master节点重启时，会从log中的最近一个checkpoint开始恢复
  + 再逐条执行从Checkpoint开始的log，最后恢复自己的状态
# GFS读文件
+ 对于读请求来说
  + 意味着应用程序或者GFS客户端有一个文件名和它想从文件的某个位置读取的偏移量（offset）
  + 应用程序会将这些信息发送给Master节点
+ Master节点会从自己的file表单中查询文件名，得到Chunk ID的数组
  + 因为每个Chunk是64MB，所以偏移量除以64MB就可以从数组中得到对应的Chunk ID
  + 之后Master再从Chunk表单中找到存有Chunk的服务器列表，并将列表返回给客户端
    + 所以，第一步是客户端（或者应用程序）将文件名和偏移量发送给Master
    + 第二步，Master节点将Chunk Handle（也就是ID，记为H）和服务器列表发送给客户端
+ 现在客户端可以从这些Chunk服务器中挑选一个来读取数据
  + 客户端会选择一个网络上最近的服务器，并将读请求发送到那个服务器
  + 因为客户端每次可能只读取1MB或者64KB数据
    + 所以，客户端可能会连续多次读取同一个Chunk的不同位置
    + 所以，客户端会缓存Chunk和服务器的对应关系
      + 这样，当再次读取相同Chunk数据时，就不用一次次的去向Master请求相同的信息
+ 接下来，客户端会与选出的Chunk服务器通信，将Chunk Handle和偏移量发送给那个Chunk服务器
  + Chunk服务器会在本地的硬盘上，将每个Chunk存储成独立的Linux文件，并通过普通的Linux文件系统管理
  + Chunk文件会按照Handle（也就是ID）命名
    + 所以，Chunk服务器需要做的就是根据文件名找到对应的Chunk文件
    + 之后从文件中读取对应的数据段，并将数据返回给客户端
# GFS写文件（Write File）
+ 写文件是，应用程序会告诉库函数说
  + 我想向这个文件名对应的文件追加数据，请告诉我文件中最后一个Chunk的位置
  + 这就是GFS论文中讨论的记录追加（Record Append）
+ 当有多个客户端同时写同一个文件时，一个客户端并不能知道文件究竟有多长
  + 例如，不同客户端写同一份日志文件，没有一个客户端会知道文件究竟有多长
  + 因此也就不知道该往什么样的偏移量，或者说向哪个Chunk去追加数据
  + 这个时候，客户端可以向Master节点查询哪个Chunk服务器保存了文件的最后一个Chunk
+ 对于读文件来说，可以从任何最新的Chunk副本读取数据
+ 但是对于写文件来说，必须要通过Chunk的主副本（Primary Chunk）来写入
  + 对于某个特定的Chunk来说，在某一个时间点，Master不一定指定了Chunk的主副本
  + 所以，写文件的时候，需要考虑Chunk的主副本不存在的情况
+ 对于Master节点来说，如果发现Chunk的主副本不存在，Master会找出所有存有Chunk最新副本的Chunk服务器
  + 如果你的一个系统已经运行了很长时间，那么有可能某一个Chunk服务器保存的Chunk副本是旧的
  + 导致这个现象的原因可能是:
    + 服务器因为宕机而没有收到任何的更新
  + 所以，Master节点需要能够在Chunk的多个副本中识别出，哪些副本是新的，哪些是旧的
    + 所以第一步是，找出新的Chunk副本
      + 这一切都是在Master节点发生
      + 因为，现在是客户端告诉Master节点说要追加某个文件
      + Master节点需要告诉客户端向哪个Chunk服务器（也就是Primary Chunk所在的服务器）去做追加操作
      + 所以，Master节点的部分工作就是弄清楚在追加文件时，客户端应该与哪个Chunk服务器通信
+ 每个Chunk可能同时有多个副本
  + 最新的副本是指，副本中保存的版本号与Master中记录的Chunk的版本号一致
  + Chunk副本中的版本号是由Master节点下发的
    + 所以Master节点知道，对于一个特定的Chunk，哪个版本号是最新的
    + 这就是为什么Chunk的版本号在Master节点上需要保存在磁盘这种非易失的存储中的原因
      + 因为如果版本号在故障重启中丢失，且部分Chunk服务器持有旧的Chunk副本
      + 这时，Master是没有办法区分哪个Chunk服务器的数据是旧的，哪个Chunk服务器的数据是最新的
+ 回到之前的话题，当客户端想要对文件进行追加，但是又不知道文件尾的Chunk对应的Primary在哪时
  + Master会等所有存储了最新Chunk版本的服务器集合完成
  + 然后挑选一个作为Primary，其他的作为Secondary
+ 之后，Master会增加版本号，并将版本号写入磁盘，这样就算故障了也不会丢失这个数据
+ 接下来，Master节点会向Primary和Secondary副本对应的服务器发送消息并告诉它们
  + 谁是Primary，谁是Secondary，Chunk的新版本是什么
  + Primary和Secondary服务器都会将版本号存储在本地的磁盘中
    + 这样，当它们因为电源故障或者其他原因重启时，它们可以向Master报告本地保存的Chunk的实际版本号
+ 所以，现在我们有了一个Primary
  + 它可以接收来自客户端的写请求，并将写请求应用在多个Chunk服务器中
  + 之所以要管理Chunk的版本号
    + 是因为这样Master可以将实际更新Chunk的能力转移给Primary服务器
    + 并且在将版本号更新到Primary和Secondary服务器之后
    + 如果Master节点故障重启，还是可以在相同的Primary和Secondary服务器上继续更新Chunk
  + 现在，Master节点通知Primary和Secondary服务器
    + 你们可以修改这个Chunk
    + 它还给Primary一个租约
      + 这个租约告诉Primary说，在接下来的60秒中，你将是Primary
      + 60秒之后你必须停止成为Primary
    + 这种机制可以确保我们不会同时有两个Primary
<img src=".\picture\image4.png">

+ 假设现在Master节点告诉客户端谁是Primary，谁是Secondary
+ GFS提出了一种聪明的方法来实现写请求的执行序列
  + 客户端会将要追加的数据发送给Primary和Secondary服务器，这些服务器会将数据写入到一个临时位置
  + 所以最开始，这些数据不会追加到文件中
  + 当所有的服务器都返回确认消息说，已经有了要追加的数据，客户端会向Primary服务器发送一条消息说
    + 你和所有的Secondary服务器都有了要追加的数据，现在我想将这个数据追加到这个文件中
+ Primary服务器或许会从大量客户端收到大量的并发请求
  + Primary服务器会以某种顺序，一次只执行一个请求
  + 对于每个客户端的追加数据请求（也就是写请求）
    + Primary会查看当前文件结尾的Chunk，并确保Chunk中有足够的剩余空间
    + 然后将客户端要追加的数据写入Chunk的末尾
    + 并且，Primary会通知所有的Secondary服务器也将客户端要追加的数据写入在它们自己存储的Chunk末尾
  + 这样，包括Primary在内的所有副本，都会收到通知将数据追加在Chunk的末尾
+ 但是对于Secondary服务器来说，它们可能可以执行成功，也可能会执行失败
  + 比如说磁盘空间不足，比如说故障了，比如说Primary发出的消息网络丢包
  + 如果Secondary实际真的将数据写入到了本地磁盘存储的Chunk中
    + 它会回复“yes”给Primary
  + 如果所有的Secondary服务器都成功将数据写入，并将“yes”回复给了Primary，并且Primary也收到了这些回复
    + Primary会向客户端返回写入成功
  + 如果至少一个Secondary服务器没有回复Primary，或者回复了，但是内容却是：
    + 抱歉，一些不好的事情发生了，比如说磁盘空间不够，或者磁盘故障了
    + Primary会向客户端返回写入失败
# GFS的一致性
+ 当我们追加数据时，面对Chunk的三个副本，当客户端发送了一个追加数据的请求
+ 要将数据A追加到文件末尾，所有的三个副本，包括一个Primary和两个Secondary
+ 都成功的将数据追加到了Chunk，所以Chunk中的第一个记录是A
<img src=".\picture\image5.png">

+ 假设第二个客户端加入进来，想要追加数据B，但是由于网络问题发送给某个副本的消息丢失了
+ 所以，追加数据B的消息只被两个副本收到，一个是Primary，一个是Secondary
+ 这两个副本都在文件中追加了数据B，所以，现在我们有两个副本有数据B，另一个没有
<img src=".\picture\image6.png">

+ 之后，第三个客户端想要追加数据C，并且第三个客户端记得下图中左边第一个副本是Primary
+ Primary选择了偏移量，并将偏移量告诉Secondary，将数据C写在Chunk的这个位置。三个副本都将数据C写在这个位置
<img src=".\picture\image7.png">

+ 对于数据B来说，客户端会收到写入失败的回复，客户端会重发写入数据B的请求
+ 所以，第二个客户端会再次请求追加数据B
  + 或许这次数据没有在网络中丢包，并且所有的三个副本都成功追加了数据B
+ 现在三个副本都在线，并且都有最新的版本号
<img src=".\picture\image8.png">

+ 之后，如果一个客户端读文件，读到的内容取决于读取的是Chunk的哪个副本
+ 客户端总共可以看到三条数据，但是取决于不同的副本，读取数据的顺序是不一样的
  + 如果读取的是第一个副本，那么客户端可以读到A、B、C，然后是一个重复的B
  + 如果读取的是第三个副本，那么客户端可以读到A，一个空白数据，然后是C、B
  + 如果读取前两个副本，B和C的顺序是先B后C，如果读的是第三个副本，B和C的顺序是先C后B
+ 或许最坏的情况是，一些客户端写文件时
  + 因为其中一个Secondary未能成功执行数据追加操作，客户端从Primary收到写入失败的回复
  + 在客户端重新发送写文件请求之前，客户端就故障了
  + 所以，你有可能进入这种情形：
    + 数据D出现在某些副本中，而其他副本则完全没有
<img src=".\picture\image9.png">

+ 在GFS的这种工作方式下，如果Primary返回写入成功，那么一切都还好
+ 如果Primary返回写入失败，就不是那么好了
+ Primary返回写入失败会导致不同的副本有完全不同的数据
---------------------------------------
+ GFS这样设计的理由是足够的简单，但是同时也给应用程序暴露了一些奇怪的数据
+ 这里希望为应用程序提供一个相对简单的写入接口，但应用程序需要容忍读取数据的乱序
+ 如果应用程序不能容忍乱序
  + 应用程序要么可以通过在文件中写入序列号，这样读取的时候能自己识别顺序
  + 要么如果应用程序对顺序真的非常敏感那么对于特定的文件不要并发写入
    + 例如，对于电影文件，你不会想要将数据弄乱
    + 当你将电影写入文件时，你可以只用一个客户端连续顺序而不是并发的将数据追加到文件中
+ 如果你想要将GFS升级成强一致系统，我可以为你列举一些你需要考虑的事情：
  + 你可能需要让Primary来探测重复的请求
    + 这样第二个写入数据B的请求到达时，Primary就知道，我们之前看到过这个请求，可能执行了也可能没执行成功
    + Primay要尝试确保B不会在文件中出现两次
    + 所以首先需要的是探测重复的能力
  + 对于Secondary来说，如果Primay要求Secondary执行一个操作，Secondary必须要执行而不是只返回一个错误给Primary
    + 对于一个严格一致的系统来说，是不允许Secondary忽略Primary的请求而没有任何补偿措施的
    + 所以我认为，Secondary需要接受请求并执行它们
    + 如果Secondary有一些永久性故障，例如磁盘被错误的拔出了，你需要有一种机制将Secondary从系统中移除
    + 这样Primary可以与剩下的Secondary继续工作
    + 但是GFS没有做到这一点，或者说至少没有做对
  + 当Primary要求Secondary追加数据时
    + 直到Primary确信所有的Secondary都能执行数据追加之前，Secondary必须小心不要将数据暴露给读请求
    + 所以对于写请求，你或许需要多个阶段
      + 在第一个阶段，Primary向Secondary发请求，要求其执行某个操作，并等待Secondary回复说能否完成该操作，这时Secondary并不实际执行操作
      + 在第二个阶段，如果所有Secondary都回复说可以执行该操作，这时Primary才会说，好的，所有Secondary执行刚刚你们回复可以执行的那个操作
    + 这是现实世界中很多强一致系统的工作方式，这被称为两阶段提交（Two-phase commit）
  + 另一个问题是，当Primary崩溃时，可能有一组操作由Primary发送给Secondary，Primary在确认所有的Secondary收到了请求之前就崩溃了
    + 当一个Primary崩溃了，一个Secondary会接任成为新的Primary
    + 但是这时，新Primary和剩下的Secondary会在最后几个操作有分歧，因为部分副本并没有收到前一个Primary崩溃前发出的请求
    + 所以，新的Primary上任时，需要显式的与Secondary进行同步，以确保操作历史的结尾是相同的
  + 最后，时不时的，Secondary之间可能会有差异，或者客户端从Master节点获取的是稍微过时的Secondary
    + 系统要么需要将所有的读请求都发送给Primary，因为只有Primary知道哪些操作实际发生了
    + 要么对于Secondary需要一个租约系统，就像Primary一样
    + 这样就知道Secondary在哪些时间可以合法的响应客户端
---------------------------------------------------
它最严重的局限可能在于，它只有一个Master节点，会带来以下问题：
+ Master节点必须为每个文件，每个Chunk维护表单
  + 随着GFS的应用越来越多，这意味着涉及的文件也越来越多
  + 最终Master会耗尽内存来存储文件表单
  + 你可以增加内存，但是单台计算机的内存也是有上限的
  + 所以，这是人们遇到的最早的问题
+ 除此之外，单个Master节点要承载数千个客户端的请求
  + 而Master节点的CPU每秒只能处理数百个请求，尤其Master还需要将部分数据写入磁盘
  + 很快，客户端数量超过了单个Master的能力
+ 另一个问题是，应用程序发现很难处理GFS奇怪的语义（本节最开始介绍的GFS的副本数据的同步，或者可以说不同步）
+ 最后一个问题是，从我们读到的GFS论文中，Master节点的故障切换不是自动的
  + GFS需要人工干预来处理已经永久故障的Master节点，并更换新的服务器，这可能需要几十分钟甚至更长的而时间来处理
  + 对于某些应用程序来说，这个时间太长了






