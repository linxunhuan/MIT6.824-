# Split Brain
+ 具备容错特性（fault-tolerant）的系统有一个共同的特点
  + MapReduce复制了计算，但是复制这个动作，或者说整个MapReduce被一个单主节点控制
  + GFS以主备（primary-backup）的方式复制数据
    + 它会实际的复制文件内容
    + 但是它也依赖一个单主节点，来确定每一份数据的主拷贝的位置
  + VMware FT，它在一个Primary虚机和一个Backup虚机之间复制计算相关的指令
    + 但是，当其中一个虚机出现故障时，为了能够正确的恢复
    + 需要一个Test-and-Set服务来确认，Primary虚机和Backup虚机只有一个能接管计算任务
+ 这三个例子中，它们都是一个多副本系统（replication system），它们存在一个共性：
  + 它们需要一个单节点来决定，在多个副本中，谁是主（Primary）
    + 使用一个单节点的好处是:
      + 它不可能否认自己
      + 因为只有一个节点，它的决策就是整体的决策
    + 但是使用单节点的缺点是:
      + 它本身又是一个单点故障（Single Point of Failure）
+ 前面介绍的这些系统，它们将系统容错的关键点，转移到了这个单点上
  + 这个单点，会在系统出现局部故障时，选择数据的主拷贝来继续工作
  + 使用单点的原因是:
    + 我们需要避免脑裂（Split-Brain）
--------------------------------------------
+ 假设我们将VMware FT中的Test-and-Set服务构建成多副本的
+ 之前这是一个单点服务，而VMware FT依赖这个Test-and-Set服务来确定Primary虚机
  + 所以，为了提高系统的容错性，我们来构建一个多副本的Test-and-Set服务
+ 现在，我们来假设我们有一个网络，这个网络里面有两个服务器（S1，S2）
  + 这两个服务器都是我们Test-and-Set服务的拷贝
  + 这个网络里面还有两个客户端（C1，C2），它们需要通过Test-and-Set服务确定主节点是谁
  + 在这个例子中，这两个客户端本身就是VMware FT中的Primary和Backup虚拟机
<img src=".\picture\image21.png">

+ 如果这是一个Test-and-Set服务，那么你知道这两个服务器中的数据记录将从0开始
  + 任意一个客户端发送Test-and-Set指令，这个指令会将服务器中的状态设置成1
  + 所以在这个图里面，两个服务器都应该设置成1，然后将旧的值0，返回给客户端
    + 本质上来说，这是一种简化了的锁服务
+ 当一个客户端可以与其中一个服务器通信，但是不能与另一个通信时，有可能出现脑裂的问题
  + 我们假设，客户端发送请求时，它会将请求同时发送给两个服务器
    + 这样，我们就需要考虑，当某个服务器不响应时，客户端该怎么做？
    + 或者说，某个服务器不响应时，整个系统该如何响应？
    + 更具体点，我们假设C1可以访问S1但是不能访问S2，系统该如何响应？
+ 一种情况是，我们必然不想让C1只与S1通信
  + 因为，如果我们只将C1的请求设置给S1，而不设置给S2，会导致S2的数据不一致
  + 所以，我们或许应该规定，对于任何操作，客户端必须总是与两个服务器交互，而不是只与其中一个服务器交互
  + 但是这是一个错误的想法
    + 因为这里根本就没有容错
    + 因为当两个服务器中的一个故障了或者失联了，我们的系统就不能工作了
      + 现在我们有两个服务器，并且两个服务器都必须一致在线，这里的难度比单个服务器更大
      + 如果这种方式不是容错的，我们需要一种行之有效的方法
+ 另一个明显的答案是，如果客户端不能同时与两个服务器交互，那它就与它能连通的那个服务器交互，同时认为另一个服务器已经关机了
  + 这也是一个错误的答案呢
  + 因为，我们的故障场景是:另一个服务器其实还开机着
  + 实际可能是网络线路出现了故障，从而导致C1可以与S1交互，但是不能与S2交互
  + 同时，C2可以与S2交互，但是不能与S1交互
  + 现在我们规定——如果一个客户端连接了两个服务器，为了达到一定的容错性，客户端只与其中一个服务器交互也应该可以正常工作
  + 但是这样就不可避免的出现了这种情况：
    + 假设这根线缆中断了，将网络分为两个部分
<img src=".\picture\image22.png">

+ C1发送Test-and-Set请求给S1，S1将自己的状态设置为1，并返回之前的状态0给C1
<img src=".\picture\image23.png">

+ 这就意味着，C1会认为自己持有锁
  + 如果这是一个VMware FT，C1对应的虚拟机会认为自己可以成为主节点
+ 但是同时，S2里面的状态仍然是0
  + 所以如果现在C2也发送了一个Test-and-Set请求，本来应该发送给两个服务器，但是现在从C2看来，S1不能访问
  + 根据之前定义的规则，那就发送给S2吧
  + 同样的C2也会认为自己持有了锁
  + 如果这个Test-and-Set服务被VMware FT使用，那么这两个VMware 虚机都会认为自己成为了主虚拟机而不需要与另一个虚拟机协商，所以这是一个错误的场景
+ 所以，在这种有两个拷贝副本的配置中，看起来我们只有两种选择：
  + 要么等待两个服务器响应，那么这个时候就没有容错能力
  + 要么只等待一个服务器响应，那么就会进入错误的场景，而这种错误的场景，通常被称为脑裂
+ 这基本是上世纪80年代之前要面临的挑战
+ 但是，当时又的确有多副本系统的要求
  + 例如，控制电话交换机的计算机系统，或者是运行银行系统的计算机系统
  + 当时的人们在构建多副本系统时，需要排除脑裂的可能
  + 这里有两种技术：
    + 第一种是构建一个不可能出现故障的网络
      + 实际上，不可能出现故障的网络一直在我们的身边
      + 电脑中，连接了CPU和内存的线路就是不可能出现故障的网络
      + 如果网络不会出现故障，这样就排除了脑裂的可能
    + 另一种就是人工解决问题，不要引入任何自动完成的操作
      + 默认情况下，客户端总是要等待两个服务器响应
      + 如果只有一个服务器响应，永远不要执行任何操作
      + 相应的，给运维人员打电话，让运维人员去机房检查两个服务器
      + 要么将一台服务器直接关机，要么确认一下其中一台服务器真的关机了，而另一个台还在工作
      + 所以本质上，这里把人作为了一个决策器
# 过半票决（Majority Vote）
+ 人们发现哪怕网络可能出现故障，可能出现分区，实际上是可以正确的实现能够自动完成故障切换的系统
  + 当网络出现故障，将网络分割成两半，网络的两边独自运行，且不能访问对方
  + 这通常被称为**网络分区**
<img src=".\picture\image24.png">

+ 在构建能自动恢复，同时又避免脑裂的多副本系统时，人们发现，关键点在于过半票决（Majority Vote）
  + 这是Raft论文中出现的，用来构建Raft的一个基本概念
+ 过半票决系统的第一步在于，服务器的数量要是奇数，而不是偶数
  + 例如在上图中（只有两个服务器），中间出现故障，那两边就太过对称了
    + 这里被网络故障分隔的两边，它们看起来完全是一样的，它们运行了同样的软件
    + 所以它们也会做相同的事情，这样不太好（会导致脑裂）
  + 但是，如果服务器的数量是奇数的
    + 那么当出现一个网络分割时，两个网络分区将不再对称
  + 然后为了完成任何操作
    + 例如Raft的Leader选举，例如提交一个Log条目
    + 在任何时候为了完成任何操作，你必须凑够过半的服务器来批准相应的操作
      + 这里的过半是指超过服务器总数的一半
      + 直观来看，如果有3个服务器，那么需要2个服务器批准才能完成任何的操作
+ 这里背后的逻辑是:
  + 如果网络存在分区，那么必然不可能有超过一个分区拥有过半数量的服务器
  + 例如，假设总共有三个服务器，如果一个网络分区有一个服务器，那么它不是一个过半的分区
  + 如果一个网络分区有两个服务器，那么另一个分区必然只有一个服务器
  + 因此另一个分区必然不能凑齐过半的服务器，也必然不能完成任何操作
+ 这里有一点需要明确
  + 当我们在说过半的时候，我们是在说所有服务器数量的一半，而不是当前开机服务器数量的一半
+ 在一个过半票决的系统中，如果有3台服务器，那么需要至少2台服务器来完成任意的操作
  + 换个角度来看，这个系统可以接受1个服务器的故障，任意2个服务器都足以完成操作
  + 如果你需要构建一个更加可靠的系统，那么你可以为系统加入更多的服务器
  + 所以，更通用的方程是：
```
如果系统有 2 * F + 1 个服务器，那么系统最多可以接受F个服务器出现故障，仍然可以正常工作
```

+ 通常这也被称为多数投票（quorum）系统，因为3个服务器中的2个，就可以完成多数投票
+ 前面已经提过，有关过半票决系统的一个特性就是:
  + 最多只有一个网络分区会有过半的服务器，所以我们不可能有两个分区可以同时完成操作
  + 这里背后更微妙的点在于
    + 如果你总是需要过半的服务器才能完成任何操作，同时你有一系列的操作需要完成，其中的每一个操作都需要过半的服务器来批准
    + 例如选举Raft的Leader，那么每一个操作对应的过半服务器，必然至少包含一个服务器存在于上一个操作的过半服务器中
    + 也就是说，任意两组过半服务器，至少有一个服务器是重叠的
  + 实际上，相比其他特性，Raft更依赖这个特性来避免脑裂
    + 例如，当一个Raft Leader竞选成功，那么这个Leader必然凑够了过半服务器的选票
    + 而这组过半服务器中，必然与旧Leader的过半服务器有重叠
    + 所以，新的Leader必然知道旧Leader使用的任期号（term number），因为新Leader的过半服务器必然与旧Leader的过半服务器有重叠
    + 而旧Leader的过半服务器中的每一个必然都知道旧Leader的任期号
    + 类似的，任何旧Leader提交的操作，必然存在于过半的Raft服务器中，而任何新Leader的过半服务器中，必然有至少一个服务器包含了旧Leader的所有操作
    + 这是Raft能正确运行的一个重要因素
+ 在过半票决这种思想的支持下，有两个系统基本同时被提出
  + 这两个系统指出，你可以使用这种过半票决系统，从某种程度上来解决之前明显不可能避免的脑裂问题
  + 例如，通过使用3个服务器而不是2个，同时使用过半票决策略
    + 两个系统中的一个叫做Paxos，Raft论文对这个系统做了很多的讨论
    + 另一个叫做ViewStamped Replication（VSR）
# Raft 初探
+ Raft会以库（Library）的形式存在于服务中
+ 一个基于Raft的多副本服务，那么每个服务的副本将会由两部分组成：
  + 应用程序代码
    + 应用程序代码接收RPC或者其他客户端请求
  + Raft库
    + 不同节点的Raft库之间相互合作，来维护多副本之间的操作同步
+ 应用程序通常都有状态，Raft层会帮助应用程序将其状态拷贝到其他副本节点
  + 对于一个Key-Value数据库而言，对应的状态就是Key-Value Table
  + 应用程序往下，就是Raft层
    + 所以，Key-Value数据库需要对Raft层进行函数调用，来传递自己的状态和Raft反馈的信息
<img src=".\picture\image25.png">

+ 同时，如Raft论文中的图2所示，Raft本身也会保持状态
+ Raft的状态中，最重要的就是Raft会记录操作的日志
<img src=".\picture\image26.png">

+ 对于一个拥有三个副本的系统来说，很明显我们会有三个服务器
+ 这三个服务器有完全一样的结构（上面是应用程序层，下面是Raft层）
+ 理想情况下，也会有完全相同的数据分别存在于两层（应用程序层和Raft层）中
+ 除此之外，还有一些客户端，假设我们有了客户端1（C1），客户端2（C2）等等
<img src=".\picture\image27.png">

+ 客户端就是一些外部程序代码，它们想要使用服务
  + 同时它们不知道，也没有必要知道，它们正在与一个多副本服务交互
  + 从客户端的角度来看，这个服务与一个单点服务没有区别
+ 客户端会将请求发送给当前Raft集群中的Leader节点对应的应用程序
  + 这里的请求就是应用程序级别的请求，例如一个访问Key-Value数据库的请求
  + 这些请求可能是Put也可能是Get
    + Put请求带了一个Key和一个Value，将会更新Key-Value数据库中
      + Key对应的Value
    + 而Get向当前服务请求某个Key对应的Value
<img src=".\picture\image28.png">

+ 一旦一个Put请求从客户端发送到了服务端，对于一个单节点的服务来说，应用程序会直接执行这个请求
  + 更新Key-Value表
  + 之后返回对于这个Put请求的响应
+ 但是对于一个基于Raft的多副本服务，就要复杂一些
  + 假设客户端将请求发送给Raft的Leader节点
  + 在服务端程序的内部，应用程序只会将来自客户端的请求对应的操作向下发送到Raft层
  + 并且告知Raft层，请把这个操作提交到多副本的日志（Log）中，并在完成时通知我
<img src=".\picture\image29.png">

+ 之后，Raft节点之间相互交互，直到过半的Raft节点将这个新的操作加入到它们的日志中，也就是说这个操作被过半的Raft节点复制了
<img src=".\picture\image30.png">

+ 当且仅当Raft的Leader过半节点知道了,都有了这个操作的拷贝之后
+ Raft的Leader节点中的Raft层，会向上发送一个通知到应用程序，也就是Key-Value数据库，来说明：
  + 刚刚你提交给我的操作，我已经提交给过半节点副本
  + 并且已经成功拷贝给它们了，现在，你可以真正的执行这个操作了
+ 每个副本节点的Raft层会将相同的操作提交到本地的应用程序层
  + 在本地的应用程序层，会将这个操作更新到自己的状态
  + 所以，理想情况是，所有的副本都将看到相同的操作序列，这些操作序列以相同的顺序出现在Raft到应用程序的upcall中
  + 之后它们以相同的顺序被本地应用程序应用到本地的状态中
  + 假设操作是确定的（比如一个随机数生成操作就不是确定的），所有副本节点的状态，最终将会是完全一样的
  + 图中的Key-Value数据库，就是Raft论文中说的状态（也就是Key-Value数据库的多个副本最终会保持一致）
<img src=".\picture\image31.png">

# Log 同步时序
+ 假设我们有一个客户端，服务器1是当前Raft集群的Leader
+ 同时，我们还有服务器2，服务器3
  + 这张图的纵坐标是时间，越往下时间越长
+ 假设客户端将请求发送给服务器1，这里的客户端请求就是一个简单的请求
  + 例如一个Put请求
+ 之后，服务器1的Raft层会发送一个添加日志（AppendEntries）的RPC到其他两个副本（S2，S3）
  + 现在服务器1会一直等待其他副本节点的响应，一直等到过半节点的响应返回
  + 这里的过半节点包括Leader自己
  + 所以在一个只有3个副本节点的系统中，Leader只需要等待一个其他副本节点
<img src=".\picture\image32.png">

+ 一旦过半的节点返回了响应，这里的过半节点包括了Leader自己
+ 所以在一个只有3个副本的系统中，Leader只需要等待一个其他副本节点返回对于AppendEntries的正确响应
<img src=".\picture\image33.png">

+ 当Leader收到了过半服务器的正确响应，Leader会执行（来自客户端的）请求，得到结果，并将结果返回给客户端
<img src=".\picture\image34.png">

+ 现在Leader知道过半服务器已经添加了Log，可以执行客户端请求，并返回给客户端
+ 但是服务器2还不知道这一点
  + 服务器2只知道：我从Leader那收到了这个请求
    + 但是我不知道这个请求是不是已经被Leader提交（committed）了，**这取决于我的响应是否被Leader收到**
  + 服务器2只知道，它的响应提交给了网络
    + 或许Leader没有收到这个响应，也就不会决定commit这个请求
+ 所以这里还有一个阶段
  + 一旦Leader发现请求被commit之后，它需要将这个消息通知给其他的副本
  + 所以这里有一个额外的消息
<img src=".\picture\image36.png">

+ 与此同时，服务器3可能也会将它的响应返回给Leader，尽管这个响应是有用的，但是这里不需要等待这个响应
<img src=".\picture\image35.png">

# 日志（Raft Log）
为什么Raft系统这么关注Log，Log究竟起了什么作用？
+ Raft系统之所以对Log关注这么多的一个原因是:
  + Log是Leader用来对操作排序的一种手段
+ 对于复制状态机来说，所有副本不仅要执行相同的操作，还需要用相同的顺序执行这些操作
  + 而Log与其他很多事物，共同构成了Leader对接收到的客户端操作分配顺序的机制
  + 比如说，我有10个客户端同时向Leader发出请求，Leader必须对这些请求确定一个顺序，并确保所有其他的副本都遵从这个顺序
  + 实际上，Log是一些按照数字编号的槽位（类似一个数组），槽位的数字表示了Leader选择的顺序
+ Log的另一个用途是，在一个（非Leader，也就是Follower）副本收到了操作，但是还没有执行操作时
  + 该副本需要将这个操作存放在某处，直到收到了Leader发送的新的commit号才执行
  + 所以，对于Raft的Follower来说，Log是用来存放临时操作的地方
  + Follower收到了这些临时的操作，但是还不确定这些操作是否被commit了
+ Log的另一个用途是用在Leader节点
  + Leader需要在它的Log中记录操作，因为这些操作可能需要重传给Follower
  + 如果一些Follower由于网络原因或者其他原因短时间离线了或者丢了一些消息，Leader需要能够向Follower重传丢失的Log消息
  + 所以，Leader也需要一个地方来存放客户端请求的拷贝
    + 即使对那些已经commit的请求，为了能够向丢失了相应操作的副本重传，也需要存储在Leader的Log中
+ 所有节点都需要保存Log还有一个原因:
  + 它可以帮助重启的服务器恢复状态
  + 你可能的确需要一个故障了的服务器在修复后，能重新加入到Raft集群，要不然你就永远少了一个服务器
    + 比如对于一个3节点的集群来说，如果一个节点故障重启之后不能自动加入
    + 那么当前系统只剩2个节点，那将不能再承受任何故障，所以我们需要能够重新并入故障重启了的服务器
      + 对于一个重启的服务器来说，会使用存储在磁盘中的Log
    + 每个Raft节点都需要将Log写入到它的磁盘中，这样它故障重启之后，Log还能保留
    + 而这个Log会被Raft节点用来从头执行其中的操作进而重建故障前的状态，并继续以这个状态运行
    + 所以，Log也会被用来持久化存储操作，服务器可以依赖这些操作来恢复状态
# 应用层接口
+ 假设我们的应用程序是一个key-value数据库，下面一层是Raft层
+ 在Raft集群中，每一个副本上，这两层之间主要有两个接口
--------------------------------------------
+ 第一个接口是key-value层用来转发客户端请求的接口
    + 如果客户端发送一个请求给key-value层，key-value层会将这个请求转发给Raft层
    + 并说：请将这个请求存放在Log中的某处
+ 这个接口实际上是个函数调用，称之为Start函数
  + 这个函数只接收一个参数，就是客户端请求
  + key-value层说：
    + 我接到了这个请求，请把它存在Log中，并在committed之后告诉我
<img src=".\picture\image37.png">

---------------------------------------------
+ 另一个接口是，随着时间的推移，Raft层会通知key-value层：
  + 你刚刚在Start函数中传给我的请求已经commit了
+ Raft层通知的，不一定是最近一次Start函数传入的请求
  + 例如在任何请求commit之前，可能会再有超过100个请求通过Start函数传给Raft层
+ 这个向上的接口以go channel中的一条消息的形式存在
  + Raft层会发出这个消息，key-value层要读取这个消息
  + 所以这里有个叫做applyCh的channel，通过它你可以发送ApplyMsg消息
<img src=".\picture\image38.png">

+ 当然，key-value层需要知道从applyCh中读取的消息
  + 对应之前调用的哪个Start函数，所以Start函数的返回需要有足够的信息给key-value层，这样才能完成对应
  + Start函数的返回值包括，这个请求将会存放在Log中的位置（index）
  + 这个请求不一定能commit成功，但是如果commit成功的话，会存放在这个Log位置
  + 同时，它还会返回当前的任期号（term number）和一些其它我们现在还不太关心的内容
+ 在ApplyMsg中，将会包含请求（command）和对应的Log位置（index）
+ 所有的副本都会收到这个ApplyMsg消息，它们都知道自己应该执行这个请求，弄清楚这个请求的具体含义，并将它应用在本地的状态中
  + 所有的副本节点还会拿到Log的位置信息（index），但是这个位置信息只在Leader有用
  + 因为Leader需要知道ApplyMsg中的请求究竟对应哪个客户端请求（进而响应客户端请求）
+ **不同副本的Log或许不完全一样**
  + 至少不同副本节点的Log的末尾，会短暂的不同
  + 例如，一个Leader开始发出一轮AppendEntries消息，但是在完全发完之前就故障了
    + 这意味着某些副本收到了这个AppendEntries，并将这条新Log存在本地
    + 而那些没有收到AppendEntries消息的副本，自然也不会将这条新Log存入本地
  + 所以，这里很容易可以看出，不同副本中，Log有时会不一样
+ 不过对于Raft来说，Raft会最终强制不同副本的Log保持一致
  + 长期来看，所有副本的Log会被Leader修改，直到Leader确认它们都是一致的
# Leader选举
+ 为什么Raft系统会有个Leader，为什么我们需要一个Leader？
  + 答案是，你可以不用Leader就构建一个类似的系统
  + 实际上有可能不引入任何指定的Leader，通过一组服务器来共同认可Log的顺序，进而构建一个一致系统
  + 实际上，Raft论文中引用的Paxos系统就没有Leader，所以这是有可能的
+ 有很多原因导致了Raft系统有一个Leader，其中一个最主要的是：
  + 通常情况下，如果服务器不出现故障，有一个Leader的存在，会使得整个系统更加高效
  + 因为有了一个大家都知道的指定的Leader，对于一个请求，你可以只通过一轮消息就获得过半服务器的认可
  + 对于一个无Leader的系统，通常需要一轮消息来确认一个临时的Leader，之后第二轮消息才能确认请求
  + 所以，使用一个Leader可以提升系统性能至2倍
  + 同时，有一个Leader可以更好的理解Raft系统是如何工作的
+ Raft生命周期中可能会有不同的Leader，它使用任期号（term number）来区分不同的Leader
  + Followers（非Leader副本节点）不需要知道Leader的ID，它们只需要知道当前的任期号
  + 每一个任期最多有一个Leader
  + 每个任期必然最多只有一个Leader
---------------------------------------------
## Leader是如何创建出来的呢？
+ 每个Raft节点都有一个选举定时器（Election Timer）
  + 如果在这个定时器时间耗尽之前，当前节点没有收到任何当前Leader的消息，这个节点会认为Leader已经下线，并开始一次选举
+ 开始一次选举的意思是，当前服务器会增加任期号（term number），因为它想成为一个新的Leader
  + 当前服务器会发出请求投票（RequestVote）RPC，这个消息会发给N-1个Raft节点
  + 因为Raft规定了，Leader的候选人总是会在选举时投票给自己
+ 并不是说如果Leader没有故障，就不会有选举
  + 但是如果Leader的确出现了故障，那么一定会有新的选举
  + 这个选举的前提是其他服务器还在运行，因为选举需要其他服务器的选举定时器超时了才会触发
+ 另一方面，如果Leader没有故障，我们仍然有可能会有一次新的选举
  + 比如，如果网络很慢，丢了几个心跳，或者其他原因
  + 这时，尽管Leader还在健康运行，我们可能会有某个选举定时器超时了，进而开启一次新的选举
+ 所以这意味着，如果有一场新的选举，有可能之前的Leader仍然在运行，并认为自己还是Leader
  + 例如，当出现网络分区时，旧Leader始终在一个小的分区中运行
  + 而较大的分区会进行新的选举，最终成功选出一个新的Leader
  + 这一切，旧的Leader完全不知道
# 选举定时器（Election Timer）
+ 任何一条AppendEntries消息都会重置所有Raft节点的选举定时器
  + 这样，只要Leader还在线，并且它还在以合理的速率（不能太慢）发出心跳或者其他的AppendEntries消息
  + Followers收到了AppendEntries消息，会重置自己的选举定时器，这样Leader就可以阻止任何其他节点成为一个候选人
  + 所以只要所有环节都在正常工作，不断重复的心跳会阻止任何新的选举发生
+ 如果一次选举选出了0个Leader，这次选举就失败了
  + 有一些显而易见的场景会导致选举失败，例如太多的服务器关机或者不可用了，或者网络连接出现故障
  + 这些场景会导致你不能凑齐过半的服务器，进而也不能赢得选举，这时什么事也不会发生
+ 一个导致选举失败的更有趣的场景是
  + 所有环节都在正常工作，没有故障，没有丢包，但是候选人们几乎是同时参加竞选，它们分割了选票（Split Vote）
  + 假设我们有一个3节点的多副本系统，3个节点的选举定时器几乎同超时，进而期触发选举
  + 首先，每个节点都会为自己投票
  + 之后，每个节点都会收到其他节点的RequestVote消息，因为该节点已经投票给自己了，所以它会返回反对投票
    + 这意味着，3个节点中的每个节点都只能收到一张投票（来自于自己）
    + 没有一个节点获得了过半投票，所以也就没有人能被选上
    + 接下来它们的选举定时器会重新计时，因为选举定时器只会在收到了AppendEntries消息时重置，但是由于没有Leader，所有也就没有AppendEntries消息
    + 所有的选举定时器重新开始计时，如果我们不够幸运的话，所有的定时器又会在同一时间到期，所有节点又会投票给自己，又没有人获得了过半投票，这个状态可能会一直持续下去
+ Raft不能完全避免分割选票（Split Vote），但是可以使得这个场景出现的概率大大降低
  + Raft通过为选举定时器随机的选择超时时间来达到这一点
  + 假设在某个时间，所有的节点收到了最后一条AppendEntries消息
    + 之后，Leader就故障
    + 这里假设所有的Followers在同一时间重置了它们的选举定时器，因为它们大概率在同一时间收到了这条AppendEntries消息
<img src=".\picture\image39.png">

+ 它们都重置了自己的选举定时器，这样在将来的某个时间会触发选举
  + 但是这时，它们为选举定时器选择了不同的超时时间
+ 假设故障的旧的Leader是服务器1，那么服务器2（S2），服务器3（S3）会在这个点为它们的选举定时器设置随机的超时时间
  + 假设S2的选举定时器的超时时间在这，而S3的在这
<img src=".\picture\image40.png">

+ 不同的服务器都选取了随机的超时时间，总会有一个选举定时器先超时，而另一个后超时
  + 假设S2和S3之间的差距足够大，先超时的那个节点（也就是S2）能够在另一个节点（也就是S3）超时之前，发起一轮选举
  + 并获得过半的选票，那么那个节点（也就是S2）就可以成为新的Leader
+ 这里对于选举定时器的超时时间的设置，需要注意一些细节
  + 一个明显的要求是，选举定时器的超时时间需要至少大于Leader的心跳间隔
  + 这里非常明显，假设Leader每100毫秒发出一个心跳，你最好确认所有节点的选举定时器的超时时间不要小于100毫秒，否则该节点会在收到正常的心跳之前触发选举
  + 所以，选举定时器的超时时间下限是一个心跳的间隔
    + 实际上由于网络可能丢包，这里你或许希望将下限设置为多个心跳间隔
    + 所以如果心跳间隔是100毫秒，你或许想要将选举定时器的最短超时时间设置为300毫秒，也就是3次心跳的间隔
----------------------------
## 超时时间的上限
+ 首先，这里的最大超时时间影响了系统能多快从故障中恢复
  + 因为从旧的Leader故障开始，到新的选举开始这段时间，整个系统是瘫痪了
    + 尽管还有一些其他服务器在运行，但是因为没有Leader，客户端请求会被丢弃
    + 所以，这里的上限越大，系统的恢复时间也就越长
  + 这里究竟有多重要，取决于我们需要达到多高的性能，以及故障出现的频率
    + 如果一年才出一次故障，那就无所谓了
    + 如果故障很频繁，那么我们或许就该关心恢复时间有多长
+ 另一个需要考虑的点是
  + 不同节点的选举定时器的超时时间差（S2和S3之间）必须要足够长，使得第一个开始选举的节点能够完成一轮选举
  + 这里至少需要大于发送一条RPC所需要的往返（Round-Trip）时间
  + 或许需要10毫秒来发送一条RPC，并从其他所有服务器获得响应
    + 如果这样的话，我们需要设置超时时间的上限到足够大，从而使得两个随机数之间的时间差极有可能大于10毫秒
# 可能的异常情况
一个新任的Leader如何能整理在不同副本上可能已经不一致的Log？
+ 如果Leader正在运行，并且在其运行时，系统中有过半服务器
  + Leader只需要告诉Followers，Log该是什么样子
  + Raft要求Followers必须同意并接收Leader的Log，只要Followers还能处理，它们就会全盘接收Leader在AppendEntries中发送给它们的内容，并加到本地的Log中
  + 之后再收到来自Leader的commit消息，在本地执行请求
  + 这里很难出错
+ 当Leader故障了才有可能出错
  + 例如，旧的Leader在发送消息的过程中故障了，或者新Leader在刚刚当选之后，还没来得及做任何操作就故障了
  + 在一系列故障之后，Log会是怎样？
+ 假设我们有3个服务器（S1，S2，S3），我将写出每个服务器的Log，每一列对齐之后就是Log的一个槽位
  + 这里写的值是Log条目对应的任期号，而不是Log记录的客户端请求
    + 所以第一列是槽位1，第二列是槽位2
  + 所有节点在任期3的时候记录了一个请求在槽位1，S2和S3在任期3的时候记录了一个请求在槽位2
  + 在槽位2，S1没有任何记录
<img src=".\picture\image41.png">

+ 假设S3是任期3的Leader，它收到了一个客户端请求，之后发送给其他服务器
  + 其他服务器收到了相应的AppendEntries消息，并添加Log到本地，这是槽位1的情况
+ 之后，S3从客户端收到了第二个请求，它还是需要将这个请求发送给其他服务器
+ 但是这里有三种情况：
  + 发送给S1的消息丢了
  + S1当时已经关机了
  + S3在向S2发送完AppendEntries之后，在向S1发送AppendEntries之前故障了
+ 现在，只有S2和S3有槽位2的Log
  + Leader在发送AppendEntries消息之前，总是会将新的请求加到自己的Log中（所以S3有Log），而现在AppendEntries RPC只送到了S2（所以S2有Log）
  + 这是不同节点之间Log不一样的一种最简单的场景
+ 如果现任Leader S3故障了，首先我们需要新的选举，之后某个节点会被选为新的Leader
  + 接下来会发生两件事情：
    + 新的Leader需要认识到，槽位2的请求可能已经commit了，从而不能丢弃
    + 新的Leader需要确保S1在槽位2记录与其他节点完全一样的请求
-------------------------------------------------
+ 这里还有另外一个例子需要考虑
  + 还是3个服务器
  + 我们这里有槽位10、11、12、13
  + 槽位10和槽位11类似于前一个例子
  + 在槽位12，S2有一个任期4的请求，而S3有一个任期5的请求
<img src=".\picture\image42.png">

+ 这种场景是可能发生的
+ 假设S2在槽位12时，是任期4的新Leader
  + 它收到了来自客户端的请求，将这个请求加到了自己的Log中，然后就故障了
+ 因为Leader故障了，我们需要一次新的选举
  + 这里S3可能被选上，因为它只需要从过半服务器获得认可投票，而在这个场景下，过半服务器就是S1和S3
  + 所以S3可能被选为任期5的新Leader，之后收到了来自客户端的请求，将这个请求加到自己的Log中，然后故障了
  + 之后就到了例子中的场景了
<img src=".\picture\image43.png">

+ 在槽位10的Log，3个副本都有记录，它可能已经commit了，所以我们不能丢弃它
+ 类似的在槽位11的Log，因为它被过半服务器记录了，它也可能commit了，所以我们也不能丢弃它
+ 在槽位12记录的两个Log（分别是任期4和任期5），都没有被commit，所以Raft可以丢弃它们
+ 这里没有要求必须都丢弃它们，但是至少需要丢弃一个Log
  + 因为最终你还是要保持多个副本之间的Log一致






