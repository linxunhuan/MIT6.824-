# 日志恢复
<img src=".\picture\image44.png">

+ 假设下一个任期是6
  + 同时假设S3在任期6被选为Leader
  + 在某个时刻，新Leader S3会发送任期6的第一个AppendEntries RPC，来传输任期6的第一个Log，这个Log应该在槽位13
+ 这里的AppendEntries消息实际上有两条，因为要发给两个Followers
  + 它们包含了客户端发送给Leader的请求
    + 我们现在想将这个请求复制到所有的Followers上
  + 这里的AppendEntries RPC还包含了prevLogIndex字段和prevLogTerm字段
    + 所以Leader在发送AppendEntries消息时，会附带前一个槽位的信息
    + 在我们的场景中，prevLogIndex是前一个槽位的位置，也就是12
    + prevLogTerm是S3上前一个槽位的任期号，也就是5
<img src=".\picture\image45.png">

+ 这样的AppendEntries消息发送给了Followers
  + 而Followers，收到了一个带有若干Log条目的消息，并且是从槽位13开始
  + Followers在写入Log之前，会检查本地的前一个Log条目，是否与Leader发来的有关前一条Log的信息匹配
+ 所以对于S2 它显然是不匹配的
  + S2 在槽位12已经有一个条目，但是它来自任期4，而不是任期5
    + 所以S2将拒绝这个AppendEntries，并返回False给Leader
  + S1在槽位12还没有任何Log，所以S1也将拒绝Leader的这个AppendEntries
+ 到目前位置，一切都还好
  + 我们完全不想看到的是，S2 把这条新的Log添加在槽位13
    + 因为这样会破坏Raft论文中图2所依赖的归纳特性
  + 并且隐藏S2 实际上在槽位12有一条不同的Log的这一事实
+ 为了响应Followers返回的拒绝，Leader会减小对应的nextIndex
  + 所以它现在减小了两个Followers的nextIndex
  + 这一次，Leader发送的AppendEntries消息中，prevLogIndex等于11，prevLogTerm等于3
  + 同时，这次Leader发送的AppendEntries消息包含了prevLogIndex之后的所有条目
    + 也就是S3上槽位12和槽位13的Log
<img src=".\picture\image46.png">

+ 对于S2来说，这次收到的AppendEntries消息中
  + prevLogIndex等于11，prevLogTerm等于3，与自己本地的Log匹配
  + 所以，S2会接受这个消息
+ Raft论文中的图2规定
  + 如果接受一个AppendEntries消息，那么需要首先删除本地相应的Log（如果有的话），再用AppendEntries中的内容替代本地Log
  + 所以，S2会这么做：它会删除本地槽位12的记录，再添加AppendEntries中的Log条目
  + 这个时候，S2的Log与S3保持了一致
<img src=".\picture\image47.png">

+ 但是，S1仍然有问题，因为它的槽位11是空的，所以它不能匹配这次的AppendEntries
  + 它将再次返回False
+ 而Leader会将S1对应的nextIndex变为11，并在AppendEntries消息中带上从槽位11开始之后的Log（也就是槽位11，12，13对应的Log）
  + 并且带上相应的prevLogIndex（10）和prevLogTerm（3）
+ 这次的请求可以被S1接受，并得到肯定的返回。现在它们都有了一致的Log
<img src=".\picture\image48.png">

+ 而Leader在收到了Followers对于AppendEntries的肯定的返回之后，它会增加相应的nextIndex到14
+ 在这里，Leader使用了一种备份机制来探测Followers的Log中，第一个与Leader的Log相同的位置
  + 在获得位置之后，Leader会给Follower发送从这个位置开始的，剩余的全部Log
  + 经过这个过程，所有节点的Log都可以和Leader保持一致
---------------------------------------------
+ 在刚刚的过程中，我们擦除了一些Log条目，比如我们刚刚删除了S2中的槽位12的Log。这个位置是任期4的Log
  + 现在的问题是，为什么Raft系统可以安全的删除这条记录？
    + 毕竟我们在删除这条记录时，某个相关的客户端请求也随之被丢弃了
+ 这条Log条目并没有存在于过半服务器中，因此无论之前的Leader是谁，发送了这条Log，它都没有得到过半服务器的认可
  + 因此旧的Leader不可能commit了这条记录，也就不可能将它应用到应用程序的状态中，进而也就不可能回复给客户端说请求成功了
  + 因为它没有存在于过半服务器中，发送这个请求的客户端没有理由认为这个请求被执行了，也不可能得到一个回复
    + 因为这里有一条规则就是，Leader只会在commit之后回复给客户端
    + 客户端甚至都没有理由相信这个请求被任意服务器收到了
  + 并且，Raft论文中的图2说明，如果客户端发送请求之后一段时间没有收到回复，它应该重新发送请求
    + 所以我们知道，不论这个被丢弃的请求是什么，我们都没有执行它，没有把它包含在任何状态中，并且客户端之后会重新发送这个请求
# 选举约束（Election Restriction）
哪些节点允许成为Leader？
+ 为了保证系统的正确性，并非任意节点都可以成为Leader
  + 不是说第一个选举定时器超时了并触发选举的节点，就一定是Leader
  + Raft对于谁可以成为Leader，谁不能成为Leader是有一些限制的
+ 为了证明并非任意节点都可以成为Leader，我们这里提出一个例子来证伪
  + 在这个反例中，Raft会选择拥有最长Log记录的节点作为Leader
    + 这个规则或许适用于其他系统，但是在Raft中，这条规则不适用
  + 所以，我们这里需要研究的问题是：
    + 为什么不选择拥有最长Log记录的节点作为Leader？
--------------------------------------------------
如果我们这么做了的话，我们需要更改Raft中的投票规则，让选民只投票给拥有更长Log记录的节点
+ 假设我们有3个服务器，现在服务器1（S1）有任期5，6，7的Log，服务器2和服务器3（S2和S3）有任期5，8的Log
<img src=".\picture\image49.png">

这个场景可能出现吗？
+ 让我们回退一些时间，在第二个时间点S1赢得了选举，现在它的任期号是6
  + 它收到了一个客户端请求，在发出AppendEntries之前
  + 它先将请求存放在自己的Log中，然后它就故障了，所以它没能发出任何AppendEntries消息
+ 之后它很快就故障重启了，因为它是之前的Leader，所以会有一场新的选举
  + 这次，它又被选为Leader
  + 然后它收到了一个任期7的客户端请求，将这个请求加在本地Log之后，它又故障了
+ S1故障之后，我们又有了一次新的选举，这时S1已经关机了，不能再参加选举，这次S2被选为Leader
  + 如果S2当选，而S1还在关机状态，S2会使用什么任期号呢？
  + 明显答案是8，但是为什么任期号是8而不是6呢？
+ 尽管没有写在黑板上，但是S1在任期6，7能当选，它必然拥有了过半节点的投票，过半服务器至少包含了S2，S3中的一个节点
  + **当某个节点为候选人投票时，节点应该将候选人的任期号记录在持久化存储中**
  + 所里在这里，S2或者S3或者它们两者都知道任期6和任期7的存在
  + 因此，当S1故障了，它们中至少一个知道当前的任期是8
    + 这里，只有知道了任期8的节点才有可能当选，如果只有一个节点知道，那么这个节点会赢得选举，因为它拥有更高的任期号
    + 如果S2和S3都知道当前任期是8，那么它们两者中的一个会赢得选举
  + 所以，下一个任期必然为8这个事实，依赖于不同任期的过半服务器之间必然有重合这个特点
    + 同时，也依赖任期号会通过RequestVote RPC更新给其他节点，并持久化存储，这样出现故障才不会丢失数据
  + 所以下一个任期号将会是8，S2或者S3会赢得选举
  + 不管是哪一个，新的Leader会继续将客户端请求转换成AppendEntries发给其他节点
<img src=".\picture\image50.png">
-------------------------------------

假设S1重新上线了，并且我们又有了一次新的选举，这时候可以选择S1作为Leader吗？
明显，答案是不可以的
+ 如果S1是Leader，它会通过AppendEntries机制将自己的Log强加给2个Followers
  + 它会发出AppendEntries消息来覆盖S2和S3在任期8的Log
  + 并在S2和S3中写入S1中的任期6和任期7的Log，这样所有的节点的Log才能与S1保持一致
为什么我们不能认可这样的结果呢？
+ 因为S2和S3可以组成过半服务器，所以任期8的Log已经被commit了
  + 对应的请求很可能已经执行了，应用层也很可能发送一个回复给客户端了
  + 所以我们不能删除任期8的Log
  + 因此，S1也就不能成为Leader并将自己的Log强制写入S2和S3
  + 正因为这个原因，**不能在选举的时候直接选择拥有最长Log记录的节点**
----------------------------------------
+ 在Raft论文的5.4.1，Raft有一个稍微复杂的选举限制（Election Restriction）
+ 这个限制要求，在处理别节点发来的RequestVote RPC时，需要做一些检查才能投出赞成票
  + 这里的限制是，节点只能向满足下面条件之一的候选人投出赞成票：
    + 候选人最后一条Log条目的任期号大于本地最后一条Log条目的任期号
    + 或者，候选人最后一条Log条目的任期号等于本地最后一条Log条目的任期号，且候选人的Log记录长度大于等于本地Log记录的长度
+ 如果S2收到了S1的RequestVote RPC，因为S1的最后一条Log条目的任期号是7，而S2的最后一条Log条目的任期号是8
  + 两个限制都不满足，所以S2和S3都不会给S1投赞成票
  + 即使S1的选举定时器的超时时间更短，并且先发出了RequestVote请求，除了它自己，没人会给它投票，所以它只能拿到一个选票，不能凑够过半选票
  + 如果S2或者S3成为了候选人，它们中的另一个都会投出赞成票，因为它们最后的任期号一样，并且它们的Log长度大于等于彼此（满足限制2）
  + 所以S2或者S3中的任意一个都会为另一个投票
# 快速恢复
+ 如果Log有冲突，Leader每次会回退一条Log条目
  + 但是在某些现实的场景中，每次只回退一条Log条目会花费很长很长的时间
  + 所以，现实的场景中，可能一个Follower关机了很长时间，错过了大量的AppendEntries消息
  + 这时，Leader重启了
    + 按照Raft论文中的图2，如果一个Leader重启了，它会将所有Follower的nextIndex设置为Leader本地Log记录的下一个槽位
    + 所以，如果一个Follower关机并错过了1000条Log条目，Leader重启之后，需要每次通过一条RPC来回退一条Log条目来遍历1000条Follower错过的Log记录
+ 假设我们有5个服务器，有1个Leader
  + 这个Leader和另一个Follower困在一个网络分区
  + 但是这个Leader并不知道它已经不再是Leader了
    + 它还是会向它唯一的Follower发送AppendEntries，因为这里没有过半服务器，所以没有一条Log会commit
  + 在另一个有多数服务器的网络分区中，系统选出了新的Leader并继续运行
    + 旧的Leader和它的Follower可能会记录无限多的旧的任期的未commit的Log
  + 当旧的Leader和它的Follower重新加入到集群中时，这些Log需要被删除并覆盖
    + 可能在现实中，这不是那么容易发生
+ 所以，为了能够更快的恢复日志，Raft论文在论文的5.3结尾处，对一种方法有一些模糊的描述,这里的大致思想是:
  + 让Follower返回足够的信息给Leader，这样Leader可以以任期（Term）为单位来回退，而不用每次只回退一条Log条目
  + 所以现在，在恢复Follower的Log时，如果Leader和Follower的Log不匹配，Leader只需要对每个不同的任期发送一条AppendEntries，而不用对每个不同的Log条目发送一条AppendEntries
  + 这只是一种加速策略
-------------------------------------------------
+ 将可能出现的场景分成3类
  + 为了简化，这里只画出一个Leader（S2）和一个Follower（S1）
  + S2将要发送一条任期号为6的AppendEntries消息给Follower
+ 场景1：S1没有任期6的任何Log，因此我们需要回退一整个任期的Log
<img src=".\picture\image51.png">

+ 场景2：S1收到了任期4的旧Leader的多条Log，但是作为新Leader，S2只收到了一条任期4的Log
  + 所以这里，我们需要覆盖S1中有关旧Leader的一些Log
<img src=".\picture\image52.png">

+ 场景3：S1与S2的Log不冲突，但是S1缺失了部分S2中的Log
<img src=".\picture\image53.png">

+ 可以让Follower在回复Leader的AppendEntries消息中，携带3个额外的信息，来加速日志的恢复
  + 这里的回复是指，Follower因为Log信息不匹配，拒绝了Leader的AppendEntries之后的回复
  + 这里的三个信息是指：
    + XTerm：
      + 这个是Follower中与Leader冲突的Log对应的任期号
      + Leader会在prevLogTerm中带上本地Log记录中，前一条Log的任期号
      + 如果Follower在对应位置的任期号不匹配，它会拒绝Leader的AppendEntries消息，并将自己的任期号放在XTerm中
      + 如果Follower在对应位置没有Log，那么这里会返回 -1
    + XIndex：
      + 这个是Follower中，对应任期号为XTerm的第一条Log条目的槽位号
    + XLen：
      + 如果Follower在对应位置没有Log，那么XTerm会返回-1，XLen表示空白的Log槽位数
+ 这些信息是如何在上面3个场景中，帮助Leader快速回退到适当的Log条目位置:
  + 场景1
    + Follower（S1）会返回XTerm=5，XIndex=2
    + Leader（S2）发现自己没有任期5的日志，它会将自己本地记录的，S1的nextIndex设置到XIndex
      + 也就是S1中，任期5的第一条Log对应的槽位号
    + 所以，如果Leader完全没有XTerm的任何Log，那么它应该回退到XIndex对应的位置
      + （这样，Leader发出的下一条AppendEntries就可以一次覆盖S1中所有XTerm对应的Log）
  + 场景2:
    + Follower（S1）会返回XTerm=4，XIndex=1
    + Leader（S2）发现自己其实有任期4的日志，它会将自己本地记录的S1的nextIndex设置到本地在XTerm位置的Log条目后面，也就是槽位2
    + 下一次Leader发出下一条AppendEntries时，就可以一次覆盖S1中槽位2和槽位3对应的Log
  + 场景3:
    + Follower（S1）会返回XTerm=-1，XLen=2
    + 这表示S1中日志太短了，以至于在冲突的位置没有Log条目，Leader应该回退到Follower最后一条Log条目的下一条，也就是槽位2，并从这开始发送AppendEntries消息
    + 槽位2可以从XLen中的数值计算得到
# 持久化（Persistence）
+ 从Raft论文的图2的左上角看到
  + 有些数据被标记为持久化的（Persistent）
  + 有些信息被标记为非持久化的（Volatile）
  + 持久化和非持久化的区别只在服务器重启时重要
+ 当你更改了被标记为持久化的某个数据
  + 服务器应该将更新写入到磁盘，或者其它的持久化存储中
    + 例如一个电池供电的RAM
  + 持久化的存储可以确保当服务器重启时，服务器可以找到相应的数据，并将其加载到内存中
  + 这样可以使得服务器在故障并重启后，继续重启之前的状态
-------------------------------------------
+ 如果一个服务器故障了，那简单直接的方法就是将它从集群中摘除
+ 我们需要具备从集群中摘除服务器，替换一个全新的空的服务器，并让该新服务器在集群内工作的能力
  + 因为如果一些服务器遭受了不可恢复的故障，例如磁盘故障，你绝对需要替换这台服务器
  + 同时，如果磁盘故障了，你也不能指望能从该服务器的磁盘中获得任何有用的信息
  + 所以我们的确需要能够用全新的空的服务器替代现有服务器的能力
+ 实际上，一个常见的故障是断电
  + 断电的时候，整个集群都同时停止运行
  + 这种场景下，如果我们希望我们的服务是容错的， 我们需要能够得到之前状态的拷贝，这样我们才能保持程序继续运行
  + 因此，至少为了处理同时断电的场景，我们不得不让服务器能够将它们的状态存储在某处，这样当供电恢复了之后，还能再次获取这个状态
    + 这里的状态是指，为了**让服务器在断电或者整个集群断电后，能够继续运行所必不可少的内容**
+ 在Raft论文的图2中，有且仅有三个数据是需要持久化存储的
  + 它们分别是Log、currentTerm、votedFor
## Log
+ Log是所有的Log条目
  + 当某个服务器刚刚重启，在它加入到Raft集群之前，它必须要检查并确保这些数据有效的存储在它的磁盘上
  + 服务器必须要有某种方式来发现，自己的确有一些持久化存储的状态，而不是一些无意义的数据
+ Log需要被持久化存储的原因是:
  + 这是唯一记录了应用程序状态的地方
+ Raft论文图2并没有要求我们持久化存储应用程序状态
  + 假如我们运行了一个数据库或者为VMware FT运行了一个Test-and-Set服务
  + 根据Raft论文图2，实际的数据库或者实际的test-set值，并不会被持久化存储，只有Raft的Log被存储了
  + 所以当服务器重启时，唯一能用来重建应用程序状态的信息就是存储在Log中的一系列操作，所以Log必须要被持久化存储
## votedFor
+ currentTerm和votedFor都是用来确保每个任期只有最多一个Leader
  + 在一个故障的场景中，如果一个服务器收到了一个RequestVote请求，并且为服务器1投票了，之后它故障
+ 如果它没有存储它为哪个服务器投过票
  + 当它故障重启之后，收到了来自服务器2的同一个任期的另一个RequestVote请求
    + 那么它还是会投票给服务器2
    + 因为它发现自己的votedFor是空的，因此它认为自己还没投过票
  + 现在这个服务器，在同一个任期内同时为服务器1和服务器2投了票
  + 因为服务器1和服务器2都会为自己投票，它们都会认为自己有过半选票（3票中的2票），那它们都会成为Leader
  + 现在同一个任期里面有了两个Leader
  + 这就是为什么votedFor必须被持久化存储
## currentTerm
+ 如果重启之后我们不知道任期号是什么，很难确保一个任期内只有一个Leader
+ 举个例子
<img src=".\picture\image54.png">

+ S1关机了，S2和S3会尝试选举一个新的Leader
  + 它们需要证据证明，正确的任期号是8，而不是6
  + 如果仅仅是S2和S3为彼此投票，它们不知道当前的任期号，它们只能查看自己的Log，它们或许会认为下一个任期是6（因为Log里的上一个任期是5）
  + 如果它们这么做了，那么它们会从任期6开始添加Log
+ 但是接下来，就会有问题了，因为我们有了两个不同的任期6（另一个在S1中）
  + 这就是为什么currentTerm需要被持久化存储的原因，因为它需要用来保存已经被使用过的任期号
----------------------------
+ 有些数据在Raft论文的图2中标记为非持久化的
+ 为什么服务器重启时，commitIndex、lastApplied、nextIndex、matchIndex，可以被丢弃？
  + 例如，lastApplied表示当前服务器执行到哪一步，如果我们丢弃了它的话，我们需要重复执行Log条目两次
  + （重启前执行过一次，重启后又要再执行一次），这是正确的吗？
  + 为什么可以安全的丢弃lastApplied？
+ 这里综合考虑了Raft的简单性和安全性
  + 之所以这些数据是非持久化存储的，是因为Leader可以通过检查自己的Log和发送给Followers的AppendEntries的结果，来发现哪些内容已经commit了
  + 如果因为断电，所有节点都重启了
    + Leader并不知道哪些内容被commit了，哪些内容被执行了
    + 但是当它发出AppendEntries，并从Followers搜集回信息
      + 它会发现，Followers中有哪些Log与Leader的Log匹配，因此也就可以发现，在重启前，有哪些被commit了
+ 另外，Raft论文的图2假设，应用程序状态会随着重启而消失
  + 所以图2认为，既然Log已经持久化存储了，那么应用程序状态就不必再持久化存储
  + 因为在图2中，Log从系统运行的初始就被持久化存储下来
  + 所以，当Leader重启时，Leader会从第一条Log开始，执行每一条Log条目，并提交给应用程序
  + 所以，重启之后，应用程序可以通过重复执行每一条Log来完全从头构建自己的状态
# 日志快照（Log Snapshot）


























