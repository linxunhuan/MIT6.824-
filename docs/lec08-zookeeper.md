# 线性一致（Linearizability）(1)
下面举一个例子
+ 竖线表示客户端发送了一个请求，并且这是个写请求，它将key为X的数据的值写成0
  + 这里有一个key，一个value
  + 并且请求对应于将key为X的数据设置成0的PUT操作
  + 某个时间点，服务响应了并说，好的，你的写操作完成了
<img src=".\picture\image61.png">

+ 假设这里的服务具备通知请求完成的能力，否则我们很难判断线性一致
  + 所以，我们有了某人发出的这个写请求
    + 假设有另一个写X的请求，将X写成1
    + 之后有个并发的写请求，或许比前一个请求开始的稍晚一点，将X写成2
<img src=".\picture\image62.png">

+ 假设有一些读操作
  + 其中一个读操作，在第一条竖线发起，在第二条竖线得到回复
    + 这个操作读的是key X，得到的是2
  + 之后，有来自于同一个客户端的另一个读请求
    + 但是这个请求在前一个读请求结束之后才开始，第二个读X的请求得到1
<img src=".\picture\image63.png">

这个历史记录是不是线性一致的？这里有两种可能。
+ 要么我们能构建一个序列，同时满足:
  + 序列中的请求的顺序与实际时间匹配
  + 每个读请求看到的都是序列中前一个写请求写入的值
+ 如果我们能构造这么一个序列，那么可以证明，这里的请求历史记录是线性的
+ 另一种可能是，如果将上面的规则应用之后生成了一个带环的图
  + 那么证明请求历史记录不是线性一致的
  + 对于小规模的历史记录，我们可以遍历每个请求来做判断
这里的请求历史记录是线性一致的
+ 这里的序列是:
  + 首先是将X写0的请求
  + 之后服务器收到了两个差不多时间的写操作，服务自己要为这两个写操作挑一个顺序
    + 我们可以假设，服务器先执行了将X写2的请求，之后执行读X返回2的请求，也就是第一个读X的请求
    + 下一个请求是将X写1的请求，最后一个请求是读X返回1
<img src=".\picture\image64.png">

# 线性一致（Linearizability）（2）
还有一个例子，它与第一个例子前半部分是一样的
<img src=".\picture\image65.png">

+ 假设有另一个客户端C2（下图有误，第二个C1应为C2），读X得到了1，再次读X得到了2
<img src=".\picture\image66.png">

+ 这个请求历史记录是线性一致的吗？
  + 我们要么需要构造一个序列（证明线性一致）
  + 要么需要构造一个带环的图（证明非线性一致）
+ 写X为2的请求，必须在C1读X得到2的请求之前，所以这里有个这样的箭头
+ C1读X得到2的请求必须在写X为1的请求之前
  + 否则C1的第二个读请求不可能得到1
  + 可以假设写X为1的请求很早就发生了（在写X为2的实际执行时间就发生了）
    + 但那样的话，C1的第二个读请求不能看到1，只能看到2
      + 因为第一个读请求看到的就是2
    + （通俗解释就是，因为第一个读请求看到的是2，如果后面没有一个别写请求的话，那么后面的读请求应该看到相同的结果）
  + 所以，读X得到2的请求必须在写X为1的请求之前
+ 写X为1的请求必须在任何读X得到1的请求之前，包括了C2读X得到1的请求
  + 但是，为了让C2先有读X得到1的请求，后有读X得到2的请求
  + C2的读X得到1的请求必须要在写X为2的请求之前（这样两次读才有可能是不同的值）
+ 这里就有了个环。所以不存在一个序列能满足线性一致的要求，因为我们构造了一个带环的图
<img src=".\picture\image67.png">

# 线性一致（Linearizability）（3）
+ 假设我们先写X为1，在这个请求完成之后，有另一个客户端发送了写X为2的请求，并收到了响应说，写入完成
+ 之后，有第三个客户端，发送了一个读X的请求，得到了1
<img src=".\picture\image68.png">

+ 这是线性一致系统，或者强一致系统不可能提供旧的数据的证据
+ 为什么一个系统有可能会提供旧的数据呢？
  + 或许你有大量的副本，每一个副本或许没有看到所有的写请求，或者所有的commit了的写请求
  + 所以，或许所有的副本看到第一个写请求，也就是写X为1的请求
    + 但是只有部分副本看到了第二个写请求，也就是写X为2的请求
  + 所以，当你向一个已经“拖后腿”的副本请求数据时，它仍然只有X的值为1
  + 然而客户端永远也不能在一个线性一致的系统中看到旧的数据（也就是X=1），因为一个线性一致的系统不允许读出旧的数据
+ 所以这里不是线性一致的，这里的教训是：
  + 对于读请求不允许返回旧的数据，只能返回最新的数据
  + 或者说，对于读请求，线性一致系统只能返回最近一次完成的写请求写入的值
----------------------------
<img src=".\picture\image69.png">

+ 现在有两个客户端，其中一个提交了一个写X为3的请求，之后是一个写X为4的请求
  + 同时，还有另一个客户端，在这个时间点，客户端发出了一个读X的请求，但是客户端没有收到回复
+ 在一个实际的系统实现中，可能有任何原因导致这个结果，例如：
  + Leader在某个时间故障了
  + 这个客户端发送了一个读请求，但是这个请求丢包了因此Leader没有收到这个请求
  + Leader收到了这个读请求并且执行了它，但是回复的报文被网络丢包了
  + Leader收到了请求并开始执行，在完成执行之前故障了
  + Leader执行了这个请求，但是在返回响应的时候故障了
+ 在大多数系统的客户端内部实现机制中，客户端将会重发请求
  + 或许发给一个不同的Leader，或许发送给同一个Leader
  + 客户端发送了第一个请求，之后没有收到回复并且超时之后，或许在这里发送了第二个请求
  + 之后，终于收到了一个回复
<img src=".\picture\image70.png">

+ 服务器处理重复请求的合理方式是，服务器会根据请求的唯一号或者其他的客户端信息来保存一个表
  + 这样服务器可以记住：
    + 之前看过这个请求，并且执行过它，我会发送一个相同的回复给它，因为我不想执行相同的请求两次
    + 例如，假设这是一个写请求，你不会想要执行这个请求两次
  + 第一个请求的回复可能已经被网络丢包了
    + 所以，服务器也必须要有能力能够将之前发给第一个请求的回复，再次发给第二个重复的请求
    + 所以，服务器记住了最初的回复，并且在客户端重发请求的时候将这个回复返回给客户端
      + 如果服务器这么做了，那么因为服务器或者Leader之前执行第一个读请求的时候，可能看到的是X=3
      + 那么它对于重传的请求，可能还是会返回X=3
    + 所以，我们必须要决定，这是否是一个合法的行为
+ 你可能会说，客户端在这里发送的（重传）请求，这在写X为4的请求之后，所以你这里应该返回4，而不是3
  + 这里取决于设计者，但是重传本身是一个底层的行为
    + 或许在RPC的实现里面，或许在一些库里面实现
    + 但是从客户端程序的角度来说，它只知道从第一条竖线的位置发送了一个请求
    + 并在第二条竖线的位置收到了一个回复
<img src=".\picture\image71.png">

+ 这是从客户端角度看到的所有事情
  + 所以，返回X为3是完全合法的
  + 因为这个读请求花费了一个很长的时间，它与写X为4的请求是完全并发的，而不是串行的
# Zookeeper
+ Zookeeper是一个现实世界成功的系统，是一个很多人使用的开源服务，并且集成到了很多现实世界的软件中
+ Raft实际上就是一个库
  + 可以在一些更大的多副本系统中使用Raft库
  + 但是Raft不是一个可以直接交互的独立的服务，必须要设计自己的应用程序来与Raft库交互
  + 所以这里有一个有趣的问题：
    + 是否有一些有用的，独立的，通用的系统可以帮助人们构建分布式系统？
    + 是否有这样的服务可以包装成一个任何人都可以使用的独立服务，并且极大的减轻构建分布式应用的痛苦？
+ 第一个问题是
  + 对于一个通用的服务，API应该是怎样？
  + 它们可以被认为是一个通用的协调服务（General-Purpose Coordination Service）
+ 第二个问题
  + 作为一个多副本系统，Zookeeper是一个容错的，通用的协调服务
  + 它与其他系统一样，通过多副本来完成容错
  + 如果我们有了n倍数量的服务器，是否可以为我们带来n倍的性能？
------------------------------------------------------------------
先说一下第二个问题
+ 接下来将会把Zookeeper看成一个类似于Raft的多副本系统
+ Zookeeper实际上运行在Zab之上
  + 从我们的角度来看，Zab几乎与Raft是一样的
  + 这里只看多副本系统的性能，并不关心Zookeeper的具体功能
+ 所以，现在全局来看，我们有大量的客户端，或许有数百个客户端，并且我们有一个Leader
  + 这个Leader有两层，上面一层是与客户端交互的Zookeeper
  + 下面是与Raft类似的管理多副本的Zab
    + Zab所做的工作是维护用来存放一系列操作的Log
      + 这些操作是从客户端发送过来的，这与Raft非常相似
    + 然后会有多个副本，每个副本都有自己的Log，并且会将新的请求加到Log中
<img src=".\picture\image72.png">

+ 当一个客户端发送了一个请求，Zab层会将这个请求的拷贝发送给其他的副本
  + 其他副本会将请求追加在它们的内存中的Log或者是持久化存储在磁盘上
  + 这样它们故障重启之后可以取回这些Log
<img src=".\picture\image73.png">

+ 当加入更多的服务器时，服务就会变得更慢
  + 当加入更多的服务器时，Leader几乎可以确定是一个瓶颈
    + 因为Leader需要处理每一个请求，它需要将每个请求的拷贝发送给每一个其他服务器
    + 当添加更多的服务器时，只是为现在的瓶颈（Leader节点）添加了更多的工作负载
  + 并不能通过添加服务器来达到提升性能的目的，因为新增的服务器并没有实际完成任何工作，它们只是愉快的完成Leader交代的工作，它们并没有减少Leader的工作
    + 每一个操作都经过Leader
    + 所以，在这里，随着服务器数量的增加，性能反而会降低，因为Leader需要做的工作更多了
    + 所以，在这个系统中，我们现在有这个问题：更多的服务器使得系统更慢了
+ 在现实世界中，大量的负载是读请求
  + 也就是说，读请求（比写请求）多得多
  + 比如，web页面，全是通过读请求来生成web页面，并且通常来说，写请求就相对少的多，对于很多系统都是这样的
  + 所以，或许我们可以将写请求发给Leader，但是将读请求发给某一个副本，随便任意一个副本
<img src=".\picture\image74.png">

如果我们直接将客户端的请求发送给副本，我们能得到预期的结果吗？
+ Zookeeper作为一个类似于Raft的系统
  + 如果客户端将请求发送给一个随机的副本，那个副本中肯定有一份Log的拷贝
  + 这个拷贝随着Leader的执行而变化
  + 假设在Lab3中，这个副本有一个key-value表
    + 当它收到一个读X的请求，在key-value表中会有X的某个数据
    + 这个副本可以用这个数据返回给客户端
+ 所以，功能上来说，副本拥有可以响应来自客户端读请求的所有数据
  + 这里的问题是：**没有理由可以相信，除了Leader以外的任何一个副本的数据是最新（up to date）的**
+ 这里有很多原因导致副本没有最新的数据，其中一个原因是
  + 这个副本可能不在Leader所在的过半服务器中
    + 对于Raft来说，Leader只会等待它所在的过半服务器中的其他follower对于Leader发送的AppendEntries消息的返回
      + 之后Leader才会commit消息，并进行下一个操作
      + 所以，如果这个副本不在过半服务器中，它或许永远也看不到写请求
  + 又或许网络丢包了，这个副本永远没有收到这个写请求
+ 所以，有可能Leader和过半服务器可以看见前三个请求，但是这个副本只能看见前两个请求，而错过了请求C
  + 所以从这个副本读数据可能读到一个旧的数据
<img src=".\picture\image75.png">

+ 即使这个副本看到了相应的Log条目，它可能收不到commit消息
  + Zookeeper的Zab与Raft非常相似，它先发出Log条目
  + 之后，当Leader收到了过半服务器的回复，Leader会发送commit消息
  + 然后这个副本可能没有收到这个commit消息
--------------------------------------------
所以，Zookeeper这里是怎么办的？
+ 实际上，Zookeeper并不要求返回最新的写入数据
+ Zookeeper的方式是，放弃线性一致性
  + 它对于这里问题的解决方法是，不提供线性一致的读
  + 它有自己有关一致性的定义，而这个定义不是线性一致的，因此允许为读请求返回旧的数据
  + 所以，Zookeeper这里声明，自己最开始就不支持线性一致性，来解决这里的技术问题
# 一致保证（Consistency Guarantees）
Zookeeper有两个主要的保证
+ 第一个是，写请求是线性一致的
  + Zookeeper只考虑写，不考虑读
  + 单独看写请求是线性一致的
+ Zookeeper的另一个保证是
  + **任何一个客户端的请求，都会按照客户端指定的顺序来执行**
    + 论文里称之为FIFO（First In First Out）客户端序列
  + 客户端是可以发送异步的写请求，也就是说客户端可以发送多个写请求给Zookeeper Leader节点，而不用等任何一个请求完成
  + 为了让Leader可以实际的按照客户端确定的顺序执行写请求
    + 客户端实际上会对它的写请求打上序号，表明它先执行这个，再执行这个，第三个是这个
    + 而Zookeeper Leader节点会遵从这个顺序
+ 对于读请求，这里会更加复杂一些
  + 读请求不需要经过Leader，只有写请求经过Leader，读请求只会到达某个副本
    + 所以，读请求只能看到那个副本的Log对应的状态
  + 对于读请求，我们应该这么考虑FIFO客户端序列
    + 客户端会以某种顺序读某个数据，之后读第二个数据，之后是第三个数据
    + 对于那个副本上的Log来说，每一个读请求必然要在Log的某个特定的点执行
    + 或者说每个读请求都可以在Log一个特定的点观察到对应的状态
<img src=".\picture\image76.png">

+ 后续的读请求，必须要在不早于当前读请求对应的Log点执行
  + 也就是一个客户端发起了两个读请求
    + 如果第一个读请求在Log中的一个位置执行
    + 那么第二个读请求只允许在第一个读请求对应的位置或者更后的位置执行
+ 第二个读请求不允许看到之前的状态
  + 第二个读请求至少要看到第一个读请求的状态
  + 这是一个极其重要的事实，我们会用它来实现正确的Zookeeper应用程序。
+ 如果一个客户端正在与一个副本交互，客户端发送了一些读请求给这个副本，之后这个副本故障了，客户端需要将读请求发送给另一个副本
  + 这时，尽管客户端切换到了一个新的副本，FIFO客户端序列仍然有效
  + 所以这意味着，如果你知道在故障前
    + 客户端在一个副本执行了一个读请求并看到了对应于Log中这个点的状态
  + 当客户端切换到了一个新的副本并且发起了另一个读请求
    + 假设之前的读请求在这里执行
    + 那么尽管客户端切换到了一个新的副本，客户端的在新的副本的读请求，必须在Log这个点或者之后的点执行
<img src=".\picture\image77.png">

+ 这里工作的原理是:
  + 每个Log条目都会被Leader打上zxid的标签，这些标签就是Log对应的条目号
  + 任何时候一个副本回复一个客户端的读请求
    + 首先这个读请求是在Log的某个特定点执行的
    + 其次回复里面会带上zxid，对应的就是Log中执行点的前一条Log条目号
  + 客户端会记住最高的zxid，当客户端发出一个请求到一个相同或者不同的副本时，它会在它的请求中带上这个最高的zxid
  + 这样，其他的副本就知道，应该至少在Log中这个点或者之后执行这个读请求
  + 这里有个有趣的场景，如果第二个副本并没有最新的Log，当它从客户端收到一个请求，客户端说，上一次我的读请求在其他副本Log的这个位置执行
<img src=".\picture\image78.png">

+ 那么在获取到对应这个位置的Log之前，这个副本不能响应客户端请求
+ 最终，如果这个副本连上了Leader，它会更新上最新的Log
  + 到那个时候，这个副本就可以响应读请求了
  + 所以读请求都是有序的，它们的顺序与时间正相关
+ 更进一步，FIFO客户端请求序列是对一个客户端的所有读请求，写请求生效
  + 所以，如果发送一个写请求给Leader，在Leader commit这个请求之前需要消耗一些时间
    + 所以现在给Leader发了一个写请求，而Leader还没有处理完它，或者commit它
  + 之后，发送了一个读请求给某个副本
    + 这个读请求需要暂缓一下，以确保FIFO客户端请求序列
  + 读请求需要暂缓，直到这个副本发现之前的写请求已经执行了
    + 这是FIFO客户端请求序列的必然结果，（对于某个特定的客户端）读写请求是线性一致的
---------------------------------
+ 如果一个客户端写了一份数据
  + 例如向Leader发送了一个写请求，之后立即读同一份数据，并将读请求发送给了某一个副本
  + 那么客户端需要看到自己刚刚写入的值
+ 如果写了某个变量为17，那么之后读这个变量，返回的不是17
  + 这表明系统并没有执行我的请求
  + 因为如果执行了的话，写请求应该在读请求之前执行
  + 所以，副本必然有一些有意思的行为来暂缓客户端
    + 比如当客户端发送一个读请求说，我上一次发送给Leader的写请求对应了zxid是多少
    + 这个副本必须等到自己看到对应zxid的写请求再执行读请求
# 同步操作（sync）
为什么Zookeeper不是一个坏的编程模型？
## 原因一：有一个弥补（非严格线性一致）的方法
+ Zookeeper有一个操作类型是sync，它本质上就是一个写请求
+ 我想读出Zookeeper中最新的数据
  + 这个时候，发送一个sync请求，它的效果相当于一个写请求，
<img src=".\picture\image79.png">

+ 它最终会出现在所有副本的Log中，尽管我只关心与我交互的副本，因为我需要从那个副本读出数据
  + 接下来，在发送读请求时，客户端告诉副本，在看到我上一次sync请求之前，不要返回我的读请求
+ 如果这里把sync看成是一个写请求，这里实际上符合了FIFO客户端请求序列
  + 因为读请求必须至少要看到同一个客户端前一个写请求对应的状态
  + 所以，如果我发送了一个sync请求之后，又发送了一个读请求
  + Zookeeper必须要向我返回至少是我发送的sync请求对应的状态
+ 不管怎么样，如果我需要读最新的数据，我需要发送一个sync请求，之后再发送读请求
+ 这个读请求可以保证看到sync对应的状态，所以可以合理的认为是最新的
+ 但是同时也要认识到，这是一个代价很高的操作
  + 因为我们现在将一个廉价的读操作转换成了一个耗费Leader时间的sync操作
  + 所以，如果不是必须的，那还是不要这么做
# 就绪文件（Ready file/znode）
+ 假设有另外一个分布式系统，这个分布式有一个Master节点，而Master节点在Zookeeper中维护了一个配置，这个配置对应了一些file（也就是znode）
+ 通过这个配置，描述了有关分布式系统的一些信息
  + 例如所有worker的IP地址，或者当前谁是Master
+ 现在Master在更新这个配置
  + 同时，或许有大量的客户端需要读取相应的配置，并且需要发现配置的每一次变化
  + 现在的问题是，尽管配置被分割成了多个file，我们还能有原子效果的更新吗？
+ 为什么要有原子效果的更新呢？
  + 因为只有这样，其他的客户端才能读出完整更新的配置，而不是读出更新了一半的配置
  + 这是人们使用Zookeeper管理配置文件时的一个经典场景
+ 假设Master做了一系列写请求来更新配置，那么我们的分布式系统中的Master会以这种顺序执行写请求
  + 首先我们假设有一些Ready file，就是以Ready为名字的file
    + 如果Ready file存在，那么允许读这个配置
    + 如果Ready file不存在，那么说明配置正在更新过程中，我们不应该读取配置
  + 所以，如果Master要更新配置，那么第一件事情是删除Ready file
  + 之后它会更新各个保存了配置的Zookeeper file（也就是znode），这里或许有很多的file
  + 当所有组成配置的file都更新完成之后，Master会再次创建Ready file
  + 目前为止，这里的语句都很直观，这里只有写请求，没有读请求，而Zookeeper中写请求可以确保以线性顺序执行
+ 为了确保这里的执行顺序，Master以某种方式为这些请求打上了tag，表明了对于这些写请求期望的执行顺序
  + 之后Zookeeper Leader需要按照这个顺序将这些写请求加到多副本的Log中
+ 接下来，所有的副本会履行自己的职责，按照这里的顺序一条条执行请求
  + 它们也会删除（自己的）Ready file
  + 之后执行这两个写请求，最后再次创建（自己的）Ready file
+ 对于读请求
  + 假设我们有一些worker节点需要读取当前的配置
  + 我们可以假设Worker节点首先会检查Ready file是否存在
    + 如果不存在，那么Worker节点会过一会再重试
+ 我们假设Ready file存在，并且是经历过一次重新创建
+ 这里的意思是:
  + 左边的都是发送给Leader的写请求
  + 右边是一个发送给某一个与客户端交互的副本的读请求
  + 之后，如果文件存在，那么客户端会接下来读f1和f2
+ 这里，有关FIFO客户端序列中有意思的地方是:
  + 如果判断Ready file的确存在，那么也是从与客户端交互的那个副本得出的判断
  + 所以，这里通过读请求发现Ready file存在，可以说明那个副本看到了Ready file的重新创建这个请求（由Leader同步过来的）
+ 同时，因为后续的读请求永远不会在更早的log条目号执行，必须在更晚的Log条目号执行
  + 对于与客户端交互的副本来说，如果它的log中包含了这条创建Ready file的log
  + 那么意味着接下来客户端的读请求只会在log中更后面的位置执行（下图中横线位置）
+ 所以，如果客户端看见了Ready file，那么副本接下来执行的读请求，会在Ready file重新创建的位置之后执行
  + 这意味着，Zookeeper可以保证这些读请求看到之前对于配置的全部更新
  + 所以，尽管Zookeeper不是完全的线性一致
    + 但是由于写请求是线性一致的，并且读请求是随着时间在Log中单调向前的，我们还是可以得到合理的结果
--------------------------------
+ 假设Master在完成配置更新之后创建了Ready file
+ 之后Master又要更新配置，那么最开始，它要删除Ready file，之后再执行一些写请求
+ 这里可能有的问题是，需要读取配置的客户端
  + 首先会在这个点，通过调用exist来判断Ready file是否存在
<img src=".\picture\image80.png">

+ 在这个时间点，Ready file肯定是存在的
+ 之后，随着时间的推移，客户端读取了组成配置的第一个file
  + 但是，之后在读取第二个file时，Master可能正在更新配置
<img src=".\picture\image81.png">

+ 现在客户端读到的是一个不正常的，由旧配置的f1和新配置的f2组成的配置
+ Zookeeper的API实际上设计的非常巧妙，它可以处理这里的问题
  + 之前说过，客户端会发送exists请求来查询，Ready file是否存在
  + 但是实际上，客户端不仅会查询Ready file是否存在，还会建立一个针对这个Ready file的watch
<img src=".\picture\image82.png">

+ 这意味着如果Ready file有任何变更
  + 例如，被删除了，或者它之前不存在然后被创建了，副本会给客户端发送一个通知
+ 在这个场景中，如果Ready file被删除了，副本会给客户端发送一个通知
+ 客户端在这里只与某个副本交互，所以这里的操作都是由副本完成
  + 当Ready file有变化时，副本会确保，合适的时机返回对于Ready file变化的通知
  + 在这个场景中，这些写请求在实际时间中，出现在读f1和读f2之间
+ 而Zookeeper可以保证，如果客户端向某个副本watch了某个Ready file，之后又发送了一些读请求
  + 当这个副本执行了一些会触发watch通知的请求
  + 那么Zookeeper可以确保副本将watch对应的通知，先发给客户端，再处理触发watch通知请求
  + （也就是删除Ready file的请求）
  + 在Log中位置之后才执行的读请求
+ 这里再来看看Log
  + FIFO客户端序列要求，每个客户端请求都存在于Log中的某个位置
  + 所以，最后log的相对位置如下图所示：
<img src=".\picture\image83.png">

+ 之前已经设置好了watch，Zookeeper可以保证如果某个人删除了Ready file
  + 相应的通知，会在任何后续的读请求之前，发送到客户端
  + 客户端会先收到有关Ready file删除的通知，之后才收到其他在Log中位于删除Ready file之后的读请求的响应
    + 这里意味着，删除Ready file会产生一个通知，而这个通知可以确保在读f2的请求响应之前发送给客户端
<img src=".\picture\image84.png">

+ 也就是，客户端读取配置读了一半，如果收到了Ready file删除的通知，就可以放弃这次读，再重试读了














